% -*- TeX -*- -*- Soft -*-
%Daemon> filter=err+warn
%Daemon> custom_args="-synctex=1"
%Daemon> ini=pdflatex

\documentclass{article}
\usepackage{lscape}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{a4wide}
\usepackage{gamesem}
\usepackage{pst-tree}
\usepackage{enumitem}
\usepackage{xspace}
\usepackage{pstring}
\usepackage{todonotes}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{bcprules}
\usepackage{algorithm}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{property}{Property}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{conjecture}{Conjecture}[section]
\newtheorem{example}{Example}[section]
\newtheorem{observation}{Observation}[section]


\newcommand\Nodes{N}% set of nodes
\newcommand\NodesVar{N_{\sf var}}% set of nodes
\newcommand\NodesLmd{N_\lambda}% set of nodes

\newcommand{\ghostlmd}{{\lambda\!\!\lambda}}
\newcommand{\ghostvar}{\theta}

\newcommand{\affine}{{\sf aff}}
\newcommand{\normalizing}{{\sf norm}}
\newcommand{\branching}{{\sf branch}}

%\newcommand\travset{\mathcal{T}\!rav}
\newcommand\travsetshort{\mathcal{T}\!r}
\newcommand{\travsetaffine}{{\travsetshort^\affine}}
\newcommand{\travsetbr}{{\travsetshort^\branching}}
\newcommand{\travsetnorm}{\travsetshort^\normalizing}

\newcommand{\child}{{\small \sf Ch}}

\newcommand{\rulefont}[1]{\mathbf{\sf #1}}
\newcommand{\enables}{\vdash}

\newcommand{\NodeHjByRoot}{\Nodes^{\theroot\enables^+}}

\newcommand\pathset{{\mathcal{P}aths}}
\newcommand\arth{\textsf{arth}}

\renewcommand\ie{{\it i.e.\@\xspace}}
\renewcommand\eg{{\it e.g.\@\xspace}}

\author{William Blum}
\title{Imaginary Traversals for the Untyped Lambda Calculus  [Draft] \\[12pt]
 {\small Normalizing untyped lambda-terms with traversals and on-the-fly eta-expansion.}}

\begin{document}
\maketitle
\begin{abstract}
We introduce a method to normalize untyped lambda terms (when the normal-form exists) by combining the theory of traversals, a term-tree traversing techniques inspired from Game Semantics \cite{Ong2006,BlumPhd}, with judicious use of the eta-conversion rule of the lambda calculus.

In the simply-typed lambda calculus (STLC), the theory of traversal is only defined for terms in eta-long form \cite{BlumPhd}. This transformation guarantees that application operands always exist when traversing the term. In the (finitary) untyped lambda calculus (ULC), however, eta-long expansion is undefined. Attempting to blindly apply the traversing rules of the STLC on an untyped lambda term therefore leads to situations where the traversal gets `stuck' due to an insufficient number of operands in an application.

We show how very little modifications to the traversal rules of STLC is required to handle the untyped case: (i) eta-long transformation is not needed: the original term is preserved intact (ii) we relax some structural restrictions of the STLC rules; (iii) theses relaxations induce the notion of ``imaginary'' nodes representing nodes that would exist in the abstract syntax tree of the term if some of its subterms were to be eta-expanded. These relaxations allow the traversal to proceed where the traversals of STLC would get blocked.

We then show how, by bounding the non-determinism of the free variable rule, one can effectively compute a subset of traversals that characterizes the set of paths in the beta-normal form of the term. This yields a normalization algorithm for ULC.
\end{abstract}

\section{Background}

Traversals were originally introduced in the context of higher-order recursion schemes \cite{Ong2006} used as generator of order-$0$ structures such as trees.
The notion was then extended to the more general setting of simply-typed languages and, in particular, to the simply-typed lambda calculus \cite{BlumPhd}, where the non-deterministic rule \rulenamet{IVar} is introduced to account for the presence of free variables. The relationship with Game Semantics is well understood in the typed setting: the set of traversals is in bijection with (the plays of) the interaction game denotation of a term, further the \emph{core projection} operation yields a bijection between the standard game denotation and its standard innocent game denotation (Theorem \ref{thm:correspondence}). This correspondence yields a method to evaluate beta-redexes of a simply-typed lambda term without performing the traditional $\beta$-reduction \cite{danos-head,BlumPhd,BlumGalop2008, Blum-LocalBeta2008}.

In this paper we show how the game-semantic traversals of STLC \cite{BlumPhd} naturally extend to ULC, and thus yield a method to normalize untyped lambda terms. In the absence of types, eta-long expansion is simply not an option: it would yield an infinite term. We thus opt instead for a more reasonable option: \emph{on-demand eta-expansion}. The idea stems from the following observation: attempting to apply the original STLC traversal rules on an untyped lambda term can lead to situations where the traversal gets ``stuck''. This happens when a sub-term in operator position has an insufficient number of operands to be able to resolve a parameter. For example, STLC traversals would get stuck when trying to resolved the ``$y$ argument'' of $f$ in term $(\lambda f.f)(\lambda y.y)$. The absence of $\eta$-long transform of the term is a notable difference with the traversals for recursion scheme \cite{Ong2006} or the simply-typed lambda calculus \cite{BlumPhd}. This simplifies the presentation as the traversed tree is just a direct representation of the original, unmodified, lambda term.

This adaptation of the game-semantic traversals to the untyped setting has several new ingredients that we will detail in the rest of the paper:
\begin{compactitem}
\item  The `lambda' and `variable' rules are augmented to implement eta-expansion.
 \item The eta-expansion is performed `on-the-fly`, as needed, at each point where the traditional STLC traversal would normally get stuck: whenever there is insufficiently many operands in an application to continue traversing the term.
\item The concept of ``ghost'' nodes is introduced. Ghost nodes are imaginary nodes in the computation tree that progressively appear as eta-expansions are performed on the sub-terms.
\item The computation tree itself does not get modified during eta-expansion. Ghost nodes are only added to the traversal and are only ``visible'' in the context where they get introduced.
\item The `input variable' rule, introduced in \cite{BlumPhd} to model free variables, is constrained by a quantity called the \emph{arity threshold}. This quantity, calculated in linear time from the traversal itself, limits the non-deterministic branching factor of the `input variable' rule. This guarantees that the normalization procedure terminates when the beta-normal form exists.
\end{compactitem}

The normalization algorithm presented in this paper was implemented in the HOG tool \cite{Blum-HogTool}. The last section of the paper provides some examples of terms normalized with the algorithm.

\subsection{Related work}

\subsubsection{Head linear reduction}
In 2004, Danos and Regnier showed the connection between Game Semantics and \emph{head linear reduction}. The various notions of traversals discussed in this paper, including Berezun-Jones, are all essentially methods based on evaluating the head linear reduction sequence of a lambda term. Traversals start by performing a depth-first search to locate the \emph{head occurrence} of the \emph{hoc redex}. They implement linear substitution by ``jumping'' to the node in the tree that represents the term to be substituted for the head occurrence. At this point the traversal continues as if the head occurrence in the tree was replaced by the subtree representing the argument of the redex. Since all the terms obtained in the head linear reduction sequence are made of sub-terms of the original term, there is no need for environments or closure to evaluate the beta-redex. Instead those can be replaced by some pointer mechanism to capture a given context.

Despite the connection established in this 2004 paper, it's not clear how the head linear reduction helps normalize a term. The \emph{Pointer Abstract Machine} introduced in the same paper, for instance, yields what they call ``\emph{quasi-head normal form (qhn)}'' of the term, not the normal form itself. The resulting \emph{qhn} still needs to be first reduced to head normal form (using head reduction) and then normalized by appealing to the standard $\lambda$-calculus theory that tells us that head reduction terminates. The traversals introduced in the present paper gives an effective algorithm to normalize any untyped lambda term without appealing to the head reduction strategy.

\subsubsection{Traversal of STLC}

In \cite{BlumPhd} we extended the theory of traversals to the simply-typed lambda calculus by introducing the new traversal rule \rulenamet{IVar} to model free variables present in lambda terms. (Such rule is not needed in the original presentation of traversals because higher-order recursion schemes are necessarily closed terms of ground type\cite{Ong2006}.) We then formalized the correspondence between the theory of traversals and Game semantics by establishing a bijection between traversals and the game denotation of a term. This correspondence yields a method to reduce beta-redexes in simply-typed lambda-terms. This normalization procedure was studied in \cite{BlumPhd,BlumGalop2008,Blum-HogTool,Ong-NormByTrav2015} and implemented in the HOG tool\cite{BlumGalop2008, BlumPhd}. We recall these results in section \ref{sec:traversal_correspondence_stlc}.

The work presented in the present paper can be viewed as a generalization of this work to the untyped case. Proposition \ref{prop:ulc_and_stlc_trav_coincide} shows that the two definition coincide for simply-typed terms.
A notable difference, however, is that the normalization procedure introduced here produces the beta-normal form of the term rather than its beta-eta-long normal form.

\subsubsection{Berezun-Jones traversals}
Berezun and Jones  introduced the first notion of traversals for the \emph{untyped} lambda calculus (ULC) \cite{JonesBerezunLLL}. Although they took inspiration from the Game Semantic traversals of \cite{Ong2006,BlumGalop2008}, their definition is more operational in nature and does not directly related to the game denotation of a term. Starting from an operational semantics of the untyped lambda calculus they derive a normalization method that, very much like the traversal of \cite{Ong2006, BlumPhd}, proceeds by traversing some tree representation of the term.

The tree representation used in Berezun-Jones' traversals are direct abstract syntax tree representation of the lambda-term, whereas in the present setting we use a more `compressed' form where consecutive lambdas and applications are merged into a single node. On the other hand, unlike the original definition of traversals for STLC where eta-long transformation is performed prior to traversing a term, both Berezun-Jones' traversals and the traversals introduced in the present paper require no prior syntactic transformation of the lambda term.

The Berezun-Jones traversals distinctively rely on two justification pointers: each `token' of the traversal can have one \emph{binding pointer} as well as one \emph{control pointer}. They also involve the use of a boolean parameter (the `flag') associated with every token of the traversal. In contrast to Berezun-Jones, the traversals presented in this paper necessitate a single justification pointer per node occurrence.

Another notable difference is that the normalization algorithm of Berezun-Jones requires a single traversal of the term. Our normalization algorithm, however, produces one traversal for every branch in the tree representing the beta-normal form. Each branching point corresponds to some occurrence of a variable in the normal form, and each branch corresponds to one operand of the variable. One can conceivably see how such non-deterministic branching could be eliminated by adding appropriate auxiliary pointers to implement backtracking: after the first operand is evaluated, backtrack to the operator variable in the traversal and explore the remaining operands.

\subsection{Rest of the paper}

In the remaining of this paper we will introduce basic definition from the traversal theory and introduce a new notion of traversals for the untyped lambda calculus. We briefly discuss the correspondence with game models of the Untyped Lambda Calculus \cite{KerThesis} and show how they yield an algorithm to normalize untyped lambda terms. Finally, we illustrate the normalization algorithm on some examples of lambda terms.

\section{Definitions}

\subsection{Untyped lambda calculus}

We consider the set of terms $\Lambda$ of the untyped lambda calculus constructed from the following grammar:
$$\Lambda := x\ |\ (\Lambda\ \Lambda)\ |\ \lambda x. \Lambda $$
where $x$ ranges over a countable set of variable names.

For conciseness when writing lambda terms, we will abbreviate ``consecutive'' lambda abstraction $\lambda x_1 \ldots \lambda x_n . U$ for some $n\geq 0$ and term $U$ as just
$\lambda x_1 \ldots x_n . U$. If $\overline{x}$ denote the list of variables $x_1 \ldots x_n$ we will just write $\lambda \overline{x} . U$.

\subsection{Computation tree and enabling relation}

Given an untyped lambda term $M$ we define its \defname{computation tree} as the abstract syntax tree (AST) representation of the lambda term where consecutive lambda abstractions are merged into a single node labelled by the list of bound variables and with a single child, and similarly consecutive applications are represented by a single node labelled $@$ with multiple children: one for the operator and one for each application operand. Formally:
\begin{definition}[Computation tree of an untyped term]
	Let $M$ be an untyped lambda term with variable names in $\mathcal{V}$.
\begin{itemize}
\item
	The \defname{computation tree} $\tau(M)$ is a labelled tree with labels taken from $ \{ @ \} \union \mathcal{V} \union \{ \lambda x_1 \ldots x_n \ | \ x_1 ,\ldots, x_n \in
	\mathcal{V}, n\in\nat \}$, defined inductively on the syntax of $M$:
	\begin{eqnarray*}
		\tau(\lambda \overline{x} . z s_1 \ldots s_m) &=& \lambda \overline{x}\; \langle\; \underline z\; \langle\tau(s_1),\ldots,\tau(s_m)\rangle\rangle\\
		&& \hbox {where $m\geq 0$, $z \in \mathcal{V}$}, \\
 \tau(\lambda \overline{x} . (\lambda y.t) s_1 \ldots s_m) &=& \lambda \overline{x}\; \langle\; @ \; \langle \tau(\lambda y.t),\tau(s_1),\ldots,\tau(s_m) \rangle \rangle \enspace\\
&&  \hbox{where $m\geq 1$, $y\in \mathcal{V}$}.
	\end{eqnarray*}
where the expression `$l\langle t_1, \ldots, t_m \rangle$' for $m \geq 0$ denotes a labelled tree with root labelled $l$ and $m$ ordered children trees $t_1$, \ldots, $t_m$. The underlined label $z$ represents a node label for $m>0$, and a leaf label if $m=0$.

\item We write $\Nodes$ to denote the set of nodes of the computation tree. We write $\NodesVar$ for the set of variable nodes, $\NodesLmd$ for lambda nodes, and $\Nodes_{@}$ for  application nodes.
We write $\theroot$ to denote the root of the tree.
\end{itemize}
\end{definition}

Note that any lambda term can indeed be written in one of the two forms above. In particular, applicative terms are handled by the case $n=0$ of the form $\lambda . N$ for some term $N$ where `$\lambda.$' is referred to as a ``dummy lambda''. This compact representation turns out to be useful to maintain alternation between lambda nodes (at odd level, counting from 1 onwards) and variable nodes (at even level).

Observe that the definition is similar to that of STLC \cite{Ong2006, BlumPhd} with the notable difference that the term does not get eta-long expanded prior to constructing the computation tree.

We define the \defname{enabling relation} $\enables$ between nodes of the computation tree as the relation associating each lambda node to all the variable nodes that it binds, the root of the tree to all the free variable nodes, and every variable and application node to each of their child node. We write $m \enables_k \alpha$ to indicate that a lambda node $\alpha$ is the $k$th child of a variable or application node $m$ for some $k\geq0$; and $\alpha \enables_k m$ to indicate that the node $m$ is labelled by the $k$th variable bound by $\alpha$ for $k\geq1$.

We define \defname{hereditarily enabling} as the transitive closure of the enabling relation, and we say that node $m$ \defname{hereditarily enables} node $n$ if $m \enables^+ n$ where $\enables^+$ denotes the transitive closure of $\enables$. We write $\NodeHjByRoot$ to denote the set of nodes hereditarily enabled by the root, that is the image of the tree root by the transitive closure of the enabling relation.

A consequence of the definition of the enabling relation is that every node of the tree is either hereditarily justified by the root or by some application node $@$.  Variable nodes that are hereditarily enabled by the root are called \defname{external variable nodes}, and variables hereditarily justified by an application node are called \defname{internal variable nodes}.

We defined the \defname{arity} of a node as follows: the arity of a lambda node $\lambda x_1 \cdots x_k$ for $k\geq 0$ is denoted by $|\lambda x_1 \cdots x_k|$ and is defined as $k$; the arity of a variable node $x$, denoted $|x|$ is the number of children of $x$ in the computation tree; the arity of a $@$ node is the number of its children nodes minus 1 (\ie, the number of operands in the application).

\subsection{Ghost nodes}
We extend the concept of nodes from the computation tree by introducing two infinite classes of ``imaginary'' nodes called \defname{ghost nodes}:
one class of ``ghost variable nodes'', denoted $\ghostvar$, and one class of ``ghost lambda nodes'', denoted $\ghostlmd$. By abuse of notation we will use $\ghostvar$ and $\ghostlmd$ to refer to particular elements of the two respective classes.

We refer to nodes $\Nodes$ of the computation tree as \defname{structural nodes}, and nodes in $\ghostlmd$ and $\ghostvar$ as the \defname{ghost nodes}. We will write $\NodesLmd^\ghostlmd$ as a shorthand for $\NodesLmd + \ghostlmd$
and $\NodesVar^\ghostvar$ as a shorthand for $\NodesVar + \ghostvar$.

We extend the definition of the enabling relation $\enables$ to ghost nodes by induction as follows: for every (ghost) variable or application node $m$ and for all $k>|m|$ there is a ghost lambda node $\ghostlmd$ such that $m \enables_k \ghostlmd$; for all (ghost) lambda node $\alpha$ and all $k>|\alpha|$ there is a ghost variable node $\ghostvar$ such that $\alpha \enables_k \ghostvar$.

Observe that every ghost variable and ghost lambda node is uniquely defined by its enabler node (possibly itself a ghost node) and the associated label $k\geq 1$.

Ghost nodes thus defined will be used as placeholders to respectively represent eta-expanded lambda nodes and variable nodes.

By convention ghost variables and lambda nodes are assigned arity $0$.

\subsection{Justified sequence of nodes}

A \defname{justified sequence} is a sequence of nodes from the computation tree where every node in the sequence, except the first one, has an associated link (the ``justification pointer'' or ``link'') pointing to some previous node occurrence in the sequence (called its ``justifier'') such as the two nodes are related by the enabling relation $\enables$ (the justifier node must enables the node pointing to it).

A justified sequence verifies the \defname{alternation condition} if the first node is a lambda node and subsequent nodes occurrences alternate between (i) a variable or application node (ii) a lambda node. From now on we will only consider justified sequences verifying the alternation condition.

It might be helpful here to highlight the fact that the enabling relation is \emph{statically} induced by the structure of the tree and defined on nodes of the tree, whereas the justification relation is defined on \emph{occurrences} of nodes and is specific to a given justified sequence.

We say that an occurrence of a node in a justified sequence is \defname{hereditarily justified by some other occurrence} if recursively following justification pointers starting from it leads to that other occurrence in the sequence. Because justification pointers must honor the enabling relation $\enables$ induced by the term structure, if a node occurrence $n$ is hereditarily justified by some occurrence of a node $m\in\Nodes$ then $n$ is necessarily hereditarily enabled by $m$. Further if $m$ occurs only once in the justified sequence then the occurrences hereditarily justified by $m$ are precisely the occurrences of nodes that are hereditarily \emph{enabled} by $m$.

For any justified sequence $t$ we write $t^\omega$ to denote the last occurrence in $t$, and $\jp(t)$ to denote the prefix of $t$ ending at the justifier of the last node of $t$. We say that $t$ is an \defname{extension} of justified sequence $u$ if $u$ is a strict subsequence of $t$ sharing the same justification pointers.

We use the standard game-semantic operations for justified sequences \cite{Abr02}. In particular we refer to the P-view of a justified sequence $s$, denoted $\pview{s}$, as the sub-sequence of $s$ obtained by reading the sequence backwards as follows: (i) if the node being read is a variable node then keep that node and follow its justification pointer (\ie, skip  all the occurrences ``underneath that pointer'') (ii) if the node is a lambda node then keep that node and move to the preceding node. The O-view, denoted $\oview{t}$, is defined as the dual of the P-view.

\begin{definition}[Projection]
Let $s$ be a justified sequence of nodes.

\begin{itemize}
\item Let $n$ be a node occurring in $s$ we write $s\filter n$ to denote the subsequence of $s$ obtained by keeping only nodes that are hereditarily justified by $n$ in $s$. We call it the \defname{projection of $s$} with respect to $n$;

 \item Let $A$ be a subset of nodes in $\Nodes$, we write $s \filter A$ to denote the subsequence of $s$ obtained by keeping only nodes that are in $A$;

 \item We call $s\filter\NodeHjByRoot$ the \defname{core} of $s$;

 \item We call $\pview{s\filter\NodeHjByRoot}$ the \defname{core P-view} of $s$.
\end{itemize}
\end{definition}
Observe that if the root of the tree occurs only once in $s$ then
$s \filter \theroot = s\filter\NodeHjByRoot$, where $\theroot$ denotes the occurrence of the root in $s$.



\subsection{Traversals}

\subsubsection{Definition}

\begin{definition}[ULC traversals]
We define the set of \defname{traversals} of an untyped lambda term $M$, denoted $\travset(M)$, or just abbreviated as $\travset$ when the term is clear from context, as the set of justified sequences of nodes in $\Nodes + \ghostlmd + \ghostvar$ (\ie, nodes from the computation tree augmented with ghost nodes) with associated pointers defined by induction over the rules of Table \ref{tab:trav_rules}.

A traversal that does not have any extension is a \defname{maximal traversal}.
\end{definition}

\begin{FramedTable}
\noindent {\bf PROGRAM -- Structural rules}
\begin{itemize}[]
    \item\rulenamet{Root} The singleton sequence $r$ is in $\travset$ where $r$ is the root of the tree.

    \item \rulenamet{App} If $t \cdot @$ is a traversal then so is \Pstr[0.4cm]{t \cdot (at) @  \cdot (a-at,40:0) \alpha} where $\alpha\in\NodesLmd$ is $@$'s $0$th child $\lambda$-node.

    \item \rulenamet{Lam} If $t \cdot \alpha$ is a traversal where $\alpha\in\NodesLmd$ then so is
        $t \cdot \alpha \cdot n$ where $n$
        denotes $\alpha$'s unique child and its justifier is defined as follows.
        \begin{compactitem}
            \item \rulenamet{Lam^@} If $n$ is an $@$-node then it has no justifier,
            \item \rulenamet{Lam^{\sf var}} If $n$ is a free variable node then it points
            to the only occurrence of the root in
            $\pview{t \cdot \alpha}$. If $n$ is a bound variable then it points to the only occurrence of its binder in $\pview{t\cdot \alpha}$.
        \end{compactitem}

    \item \rulenamet{Lam^\ghostlmd} If
  $\Pstr[0.5cm]{ t \cdot(alpha){\alpha} \cdot
(n){n} \cdot \ldots \cdot
(gl-n,40:i){\ghostlmd} \in\travset}$ for some prefix $t$, $\alpha \in \NodesLmd^\ghostlmd$ and $n \in\NodesVar^\ghostvar$ then
$$\Pstr[0.5cm]{ t \cdot(alpha){\alpha} \cdot
(n){n}
\cdot \ldots \cdot
(gl-n,40:i){\ghostlmd}\cdot (al-alpha,40:{|\alpha| + i - |n|}){\ghostvar}
      \in\travset}$$
 \end{itemize}

\emph{\bf PROGRAM -- Copy-cat rules \underline{with on-the-fly eta-expansion}}
\begin{itemize}
\item \rulenamet{Var} If \Pstr[0.5cm]{t \cdot (m){m} \cdot (alpha){\alpha}
    \ldots (n-alpha,50:i){n} \in \travset} for $i>0$, $n \in \NodesVar^\ghostvar$ hereditarily justified by an $@$-node;
     $m \in \NodesVar^\ghostvar \union \Nodes_{@}$; and $\alpha \in \NodesLmd^\ghostlmd$ then:
  $$\Pstr[0.5cm]{ t  \cdot
(m){m} \cdot (lx){\alpha}  \ldots (x-lx,30:i){n}
    \cdot (letai-m,40:i){\beta}
     \in\travset}$$
\begin{description}[align=right,labelwidth=2cm]
\item[Concrete] $i \leq |m|$: $\beta$ is a structural lambda node in $\NodesLmd$ and is the $i$th child of $m$;
\item[Eta-expanded] $i > |m|$: $\beta$ is a ghost lambda node $\ghostlmd$ representing the $i$th ghost child of $m$.
\end{description}
\end{itemize}

\emph{\bf DATA -- Input-variable rules}
\begin{itemize}
\item \rulenamet{IVar} If $t \cdot n$ is a traversal where $n \in \NodesVar^\ghostvar$ is hereditarily justified by the root. For every node $m \in \NodesVar^\ghostvar$ occurring in $\oview{t\cdot n}$
and every $i>1$ we have $t \cdot n \cdot \alpha \in \travset$ with $\alpha$ pointing to $m$ with label $i$, where:
\begin{description}[align=right,labelwidth=2cm]
\item[Concrete] $i\leq|m|$: $\alpha \in \Nodes_\lambda$ is the $i$th child of $m$;
\item[Eta-expanded] $i>|m|$: $\alpha$ is a ghost lambda node $\ghostlmd$.
\end{description}
\end{itemize}


\caption{Game-semantic `imaginary' traversal rules for the untyped lambda calculus.}
 \label{tab:trav_rules}
\end{FramedTable}

\begin{property}
The traversal rules are well-defined.
\end{property}
\begin{proof}
(i) Rule \rulenamet{Lam}: An easy induction shows that the P-view of a traversal ending with a structural node is precisely the path from the root of the tree to last node of the traversal (with associated justification pointers).
Therefore, if the last node is a variable node, its enabler necessarily occurs exactly once in the P-view.
(ii) Rule \rulenamet{Var}: In the concrete sub-case, $m$ is necessarily itself a structural node since $|m|\geq i>0$, and its $i$th child exists in $\Nodes$ since $m$ as arity greater than $i$.
\end{proof}

\subsubsection{Comparison with STLC}

The notion of traversals for simply-typed languages was previously studied in \cite{BlumPhd}. Given a simply-typed term-in-context $\Gamma \entail M :T$, with typed-context $\Gamma$ (a set of typed variables), and simple type $T$, its set of traversals $\travset_{STLC}(\Gamma \entail M :T)$ is a set of sequences of nodes from some tree representation of $M$ where each node occurrence in a given sequence can have an associated pointer to some previous occurrence in the sequence.
\begin{definition}[STLC traversal \cite{BlumPhd}]
Given a simply-typed term in context $\Gamma \vdash M : T$ we write $\travset_{STLC}(\Gamma \vdash M : T)$ to denote the set of traversals as defined in \cite{BlumPhd}.
\end{definition}

The traversal rules defined in Table \ref{tab:trav_rules} closely match those of the simply-typed lambda calculus or PCF \cite{BlumPhd}. Compared to PCF, there is no interpreted constants therefore rules \rulenamet{Value} and \rulenamet{InputValue} from the original presentation are not needed. Futher, there are three important differences compared to the traversal rules of STLC:
\begin{enumerate}
  \item In the original STLC traversals, the term gets eta-long expanded prior to calculating the set of traversals. This guarantees that the operand of an application always exists in the tree. In ULC, however, eta-long expansion is an infinite process, so instead of eta-long expanding the term beforehand, we eta-expand on the fly as needed. Eta-expansion occurs in rule \rulenamet{Var} when the arity of a variable in operand position is too low to be able to statically determine the operand of an application (case when $i>|n|$). When the variable arity is high enough ($i\geq|n|$), the two versions of the rule (for STLC and ULC) coincide and the static tree representation of the term dictates which node to traverse next.

  \item Similarly, the input-variable rule allows for infinitely many eta-expansions ($k>0$).

  \item There is an additional rule \rulenamet{Lam^\ghostlmd} for the case where a traversal ends with a ghost lambda node. In the concrete case, rule \rulenamet{Lam} just visit the unique child node of the last lambda node in the traversal. In the eta-expanded version where the last node is a ghost lambda node, there is no such child node, so we just create an imaginary one: the variable node that would be created if we were to eta-expand the sub-term under the lambda.
      This gives the intuition behind the value $j = |\alpha| + i - |n|$ in the rule definition: the ghost placeholder $\ghostvar$ represents an occurrence of the $j$th variable that would be bound by lambda node $\alpha$ if the sub-term at node $\alpha$ were eta-expanded $i-|n|$ times.
  (Observe that in this case we necessarily have $i>|n|$ since the $i$th child of $n$ is a ghost variable node.)
\end{enumerate}



The traversal of Table \ref{tab:trav_rules} coincide with those of \cite{BlumPhd} in the following sense:
\begin{proposition}[ULC and STLC traversals coincide]
\label{prop:ulc_and_stlc_trav_coincide}
For any simply-typed term in context $\Gamma \vdash M : T$ such that $M$ is in \emph{eta-long normal form} (\ie, every subterm is eta-expanded as many times as possible with respect to type $T$):
\begin{itemize}
\item[(i)] The traversals of $\travset(M)$ (where $M$ is considered as an untyped term) do not contain any ghost node.
\item[(ii)] $\travset_{STLC}(\Gamma \vdash M : T) = \travset(M)$.
\end{itemize}
\end{proposition}
\begin{proof}
(i) Because the term is eta-long expanded, the condition $i>|m|$ in the rules \rulenamet{Var} and \rulenamet{IVar} will never be met. (ii) The remaining traversals rules coincide for STLC and ULC.
\end{proof}

\subsubsection{Property of ghost nodes}

It is helpful to think of ghost nodes as the counterpart of complex numbers sometimes used in mathematics
to prove trigonometry identities: they are introduced intermediately to perform some computation or calculation (e.g. using De Moivre's Theorem) but do not appear in the final result. Just like the imaginary number $i$ is created out of the impossibility of calculating the square root of $-1$, ghost nodes are defined from the impossibility of ``traversing'' a beta-redex of a lambda term due to an insufficient number of operands.


Ghost nodes appear in a traversal when the arity of a node is too low to continue a
structural traversal of the tree:
\begin{property}
If $\Pstr[0.5cm]{(x){x} \cdots (y-x,30:i){y}} \in \travset$ then
$$ y \in \ghostvar \mbox{ or } y \in\ghostlmd \iff i > |x|.$$

In particular for all $n\in\NodesVar$ and $\alpha\in\NodesLmd$:
\begin{enumerate}
\item $\Pstr[0.5cm]{(x){n} \cdots (y-x,30:i){\ghostlmd}} \in \travset \implies i > |n|$
\item $\Pstr[0.5cm]{(x){\alpha} \cdots (y-x,30:i){\ghostvar}} \in \travset \implies i > |n|$.
\end{enumerate}
\end{property}

\begin{definition}
We call \defname{ghost materialization} any application of a traversal rule  where the last occurrence in the traversal before the rule is applied is a ghost nodes ($\ghostvar$ or $\ghostlmd$), and the new node traversed after applying some traversal rule is a structural node of the tree (in $\Nodes$).
\end{definition}

\begin{remark}[Only \rulenamet{Var} can materialize a ghost node]
Observe that among all the rules defined in Table \ref{tab:trav_rules}, the rule \rulenamet{Var} is the only rule that can materialize a structural node in $\Nodes$ from a traversal ending with a ghost node. This means that after traversing ghost nodes, the only way to come back to structural nodes is to visit a ghost variable node $\ghostvar$ with an application of rule \rulenamet{Var} of the following form:

$$\rulename{Var}\ \  \Pstr[0.5cm]{ t \cdot(beta){\beta} \cdot
(y){y} \cdot (l){\alpha}  \ldots (t-l,30:i){\ghostvar}
    \cdot (lx-y,40:i){\lambda \overline{x}}
     \in\travset}$$
where
\begin{itemize}
\item $y \in \NodesVar$
\item $\lambda \overline{x} \in \NodesLmd$ is the $i$th child lambda node of $y \in \NodesVar$,
\item $\alpha$ is either a structural lambda node in $\Nodes_\lambda$ or a ghost lambda node in $\ghostlmd$
\item $0\leq \alpha < i \leq |y|$.
\end{itemize}
\end{remark}

\section{Game semantic correspondence}
\label{sec:traversal_correspondence_stlc}

In \cite{BlumPhd} we formalized the correspondence between the theory of traversals and Game Semantics in the setting of simply-typed languages: there is a bijection between the set of traversals of $M$ and the revealed interaction game denotation of $M$, further there is a traversal transformation (the `core projection') that yields a bijection with the innocent game denotation of $M$. Formally:
\begin{theorem}[Game Semantics and STLC Traversals Correspondence -- (Theorem 4.96 in \cite{BlumPhd}]
\label{thm:correspondence}
For every simply-typed term $\Gamma \entail M :T$ we have the following two bijections:
\begin{eqnarray*}
 \travset_{STLC}(\Gamma \entail M : T)^\star & \cong & \revsem{\Gamma \entail M :T} \\
 \travset_{STLC}(\Gamma \entail M : T)^{\filter \theroot} & \cong & \sem{\Gamma \entail M :T} \enspace .
\end{eqnarray*}
where $\revsem{\Gamma \entail M : T}$ denotes the interaction game denotation of $M$, $\sem{\Gamma \entail M : T}$ denotes the innocent game denotation of $M$,
$\travset_{STLC}(M)^\star$ denotes the set of traversals where application nodes are removed, and $\travset_{STLC}(M)^{\filter \theroot}$ denotes the set of \emph{core projections} of $M$.
\end{theorem}

In other words, the traversal cores are precisely plays from the game denotation of the term.  The (fairly technical) proof relies on several key concepts \cite{BlumPhd}:
\begin{enumerate}
  \item The introduction of a \defname{traversal rule for free variables} of the lambda calculus \rulenamet{IVar}. Such rule is not needed in the original presentation of traversals because higher-order recursion schemes are necessarily closed terms of ground type;
  \item The notion of \defname{interaction game semantics} obtained from game denotation of a term by revealing all the internal moves when composing two game strategies;
  \item The definition of various operations and transformations on traversals, in particular the \defname{projection with respect to the root of the tree} which preserves only the ``core'' of a traversal.
\end{enumerate}

\paragraph{Path-View correspondence}
In the traversal world, the game-semantic concept of ``Proponent view'' (also shortned as ``P-view''), corresponds to the concept of path in the tree representation of the term (\cite{Ong2006}, \cite[Proposition 4.29]{BlumPhd}).


\paragraph{Game semantics Correspondence for ULC}
What would be the equivalent of Theorem \ref{thm:correspondence} in the untyped case? In his thesis, Andrew Ker defined and studied Game models for the untyped lambda calculus\cite{KerThesis}.  We conjecture that the traversal-game semantics isomorphism for STLC also yields between the ULC using traversals from Table \ref{tab:trav_rules} and the game model of ULC from Ker's thesis: we conjecture that there is an isomorphism between the set of imaginary traversals and the revealed game semantics of the ULC term, and further, an isomorphism between the standard game semantics and the set of traversal cores:

\begin{conjecture}[ULC Traversals Correspondence with Game Semantics]
\label{thm:ulc_corresp}
For every untyped lambda-term $M$ we have the two bijections:

\begin{eqnarray*}
 \travset(M)^\star &\cong& \revsem{M} \\
 \travset(M)^{\filter \theroot} &\cong& \sem{M} \enspace .
\end{eqnarray*}

where
$\sem{M}$ denotes the innocent \emph{effectively almost everywhere (EAC)} game denotation of $M$ defined in \cite{KerThesis},
and $\revsem{M}$ denotes the corresponding interaction game denotation where internal moves are not hidden during strategy composition.

$\travset(M)^\star$ denotes the set of traversals where application nodes are removed, and $\travset(M)^{\filter
\theroot}$ denotes the set of traversal cores of $M$ (\ie, traversal projected to the root of the tree).
\end{conjecture}

\begin{proof}[Proof idea]
The argument should follow the same structure as for STLC \cite{BlumPhd} but using the Ker's game model of ULC instead of the innocent game model of STLC.

%Rule \rulenamet{IVar} is defined so as to obtain the correspondence with game semantics: the nodes visited by this rule correspond to moves played by the Opponent in the game denotation. (We recall that in Game Semantics, the Opponent represents the environment of the program, or in other words, it models the larger program context in which the program is being executed. In particular, it determines the input data that is provided to the program.)

Our ``on-the-fly'' eta-expansion should correspond to the $Fun : U \rightarrow (U \Rightarrow U)$ morphism of Ker's game category where $U$ denotes the maximal arena.
\end{proof}

\section{Normalizing with Traversals}

\subsection{Normalizing STLC terms}

In STLC, as mentioned in the introduction, the Correspondence Theorem \ref{thm:ulc_corresp} yields a method to normalize a simply-typed lambda term. This is because the beta-normal form of a simply-typed term $M$ is uniquely determined by the maximal paths of its tree representation. By the Game-Traversal Correspondence Theorem, the set of traversals of the beta-normal form of $M$ is precisely the set of cores of traversals of $M$. (We recall that the \defname{projection with respect to the root} of a traversal $t$, also called the \defname{core of the traversal}, consists of the subsequence of nodes that are hereditarily justified by the initial occurrence of the root.) By the Path-P-view correspondence\cite{Ong2006,BlumPhd}, the set of paths in tree of the normal form is given by the set of P-views of traversal cores of $M$. Hence the P-views of maximal traversals of $M$ yields all the maximal paths in the tree representation of the beta-normal form of $M$.
Formally, for every simply-typed term $M$:
\begin{equation}
\pview{\travset(M)\filter\theroot} = \pathset(\betanf{M}) \label{eqn:pviewcore_paths}
\end{equation}
where $\pathset (M)$ denotes the set of maximal paths in the computation tree representation of $M$.

Because P-views are effectively computable from the traversal, this equality yields a normalization procedure for STLC:
\begin{algorithm}
\caption{Normalization by traversals}
\label{algo:stlc_normalization_by_traversals}
\begin{enumerate}
  \item Enumerate maximal branching traversals$^{(*)}$ of $M$;
  \item For each traversal, project it with respect to the tree root to get the traversal core. By the correspondence theorem this yields a play in $\sem{M}$;
  \item Calculate the P-view of the traversal core;
  \item Apply the bijection from the Correspondence Theorem to get the corresponding path in the tree representation of the $\beta$-nf of $M$;
  \item Aggregate all the paths thus obtained to reconstruct the beta-normal form of $M$.
\end{enumerate}
$^{(*)}$ The set of traversals may be infinite, one can define a finite subset of \emph{branching traversals} that is sufficient to cover all the paths in the beta-normal from (See remark below).
\end{algorithm}

\begin{remark}
By the Game Semantics correspondence, the set of STLC traversals can be infinite. Indeed, the game denotation of a term must allow Opponent's moves accounting for all possible context terms. In particular, suppose that the term $M$ is a second order term, then one can define an infinite sequence of term contexts that would call $M$ with a function parameter that would call its argument exactly $k$ times for $k>0$. It turns out that traversals accounting for all those contexts are redundant when it comes to normalizing the term. The intuition being that, due to the absence of side-effects, calling the same argument multiple times will always involves the same underlying computation in $M$. This is formalized in Proposition \ref{prop:branching_traversal_normalizing} (for ULC) which shows that the above equality can be maintained when restricting the set of traversals to the subset of \emph{branch traversals} where such repeatitive behaviour of the Opponent is forbidden. The same argument holds for STLC.
\end{remark}

\paragraph{Implementation of the normalization procedure}
This normalization algorithm was implemented in the HOG tool\cite{BlumGalop2008, BlumPhd}. This tools takes any simply-typed lambda term and lets the user interactively generate all the traversals by ``playing the traversal game'' over the tree representation of the term. The tool offers a `worksheet' environment where the user can apply various operations on the traversals, including projections, views and core.

\subsection{Normalization in ULC}

In this section we aim to establish a similar normalization procedure for the untyped lambda calculus by proving the counterpart of Equation \ref{eqn:pviewcore_paths} for the traversal definition of Table \ref{tab:trav_rules} applied to untyped lambda-terms with a normal form.

\subsection{The View-Path correspondence for ULC}

Because ULC is not strongly-normalizing, traversals are not necessarily finite. There is therefore no notion of `last occurrence' in such traversal, which means that the P-view itself is undefined. We will instead consider P-views of finitary approximations of traversals. In other words, for ULC, the counterpart of the above equation that we want to prove is:
$$ \Union_{k\geq0} \pview{\travset^k(M)\filter\theroot} = \pathset(\betanf{M}) . $$
for any untyped lambda term $M$ having a normal form $\betanf{M}$, where $\pathset (\betanf{M})$ denotes the set of \emph{possibly infinite} maximal paths in the infinitary computation tree\footnote{For a formal definition of infinitary computation trees see \cite{Blum17-HomogNotRestriction, Ong2006}} of the normal form of $M$, and  $\travset^k(M)$ denotes the set of traversals of length $k$ at most.

\subsection{Resticting non-determinism}

All the traversal rules are deterministic except the one used to model free variables: The rule \rulenamet{IVar} offers two non-deterministic choices: (i) which variable to pick in the O-view,
(ii) which child lambda node to pick amongst the picked variable's children.
A consequence of this definition is that even for terms having a normal form (and in particular even for beta-normal terms), the set of traversals can be infinite, and traversals can be infinitely long. For example, take the term $\lambda f . f (\lambda x. x)$. For all $k\geq0$, the sequence $\lambda f \cdot f \cdot (\lambda x \cdot  x)^k$ (with appropriate pointers) is a traversal. From a game semantic viewpoint, those traversals accounts for all the possible denotations of the function parameter $f$: indeed, for each $k$, there exists a term that applies its argument $k$ times: $\lambda y . y (y ( \ldots y)) $, and there got to be plays in the game denotation of the term to accounts for those possible arguments.

For the sake of computing the beta-normal form of a term, however, it's only necessary to traverse finitely many finite traversals (provided that the normal form exists). This is achieved by resticting the rule \rulenamet{IVar} to traverse only nodes leading to paths in the computation tree that are yet unexplored. In practice, this can be done as follows: when choosing the next lambda node to visit, one can restrict the choice of the next move to child lambda nodes of the \emph{latest} nodes in the traversal. This intuition is formalized by the notion of \emph{branching traversal} in Definition \ref{dfn:branching_traversals} and Proposition \ref{prop:branching_traversal_normalizing}.

\subsubsection{Context-affine traversals}

A lambda term is \defname{affine} if every bound variable in the term occurs at most once in the term.

Intuition: in the definition of traversals from Table \ref{tab:trav_rules}, the rule \rulenamet{IVar} is defined so as to account for all possible moves from the opponent O. This is needed to get the isomorphism with the game denotation of a term. For instance this is necessary to model contexts terms like the Kierstead terms $\lambda f.f(\lambda x.f(\lambda y.y))$ and $\lambda f.f(\lambda x.f(\lambda y.x))$. So for any lambda term $M$, its game denotation must contain plays with two occurrences of the same O-move representing the two separate occurrences of $f$.

In the game denotation of a lambda term $M$, at every point in a given play where it is Opponent's turn to play, all possible Opponent moves are accounted for. This guarantees that the denotation models all possible behaviours of contextual terms interacting with $M$, including behaviours not necessarily representable by a lambda term (e.g., in a typed setting, the context may range over a PCF term or Idealized Algol term).

For the sake of computing the beta normal form of a term, however, it is sufficient to only consider sufficiently enough plays to cover all paths of the tree representation of the beta-normal form of the term. For instance, we will now show that it is enough to consider plays modeling interaction with contexts where bound variables are referenced at most once.

\begin{definition}[Context-affine traversals]
For any traversal $t$ we will consider the set of triplets defining all possible O-move enabled by variable nodes in the O-view of $t$:
$$ E(t) = \left\{ (m, k, \alpha) \in \NodesVar^\ghostvar\times\nat^1\times \NodesLmd^\ghostlmd |\ \mbox{$m$ occurs in $\oview{t}$ and $m\enables_k\alpha$} \right\}
$$
and its subset representing all the justification pointers already present in the O-view of $t$:
$$ J(t) = \left\{ (m, k, \alpha) \in \NodesVar^\ghostvar\times\nat^1\times \NodesLmd^\ghostlmd
|\ \mbox{$\alpha$ is justified by $m$ in $\oview{t}$ with label $k$} \right\}
$$

We define the set of \defname{context-affine traversals} $\travsetaffine$ as the subset of $\travset$ defined by induction with the rules of Table \ref{tab:trav_rules} (substituting $\travsetaffine$ for $\travset$ everywhere) where the input-variable rule $\rulefont{IVar}$ is constrained as follows:

\infrule[$\rulefont{IVar}_\affine$]
     {
            t = t_1 \cdot m \cdot t_2 \in \travsetaffine
            \andalso t^\omega \in \NodeHjByRoot \inter \NodesVar
            \andalso (m,k,\alpha) \in E(t) \setminus J(t)
            \andalso k \geq 1
     }
     {\Pstr[0.5cm]{t_1 \cdot (m){m} \cdot t_2 \cdot (l-m,25:k){\alpha}} \in \travsetaffine}
\end{definition}


\begin{proposition}[Affine traversals are sufficient for normalization]
For every traversal $t\in\travset$ there exists an affine traversal $u \in \travsetaffine$ such that
$$\pview{u\filter\theroot} = \pview{t\filter\theroot} \ .$$
\end{proposition}
\begin{proof}
We omit the proof, it's a similar argument to the proof of Proposition \ref{prop:branching_traversal_normalizing} below.
\end{proof}


\subsubsection{Branching traversals}
We now show a stricter result: for the purpose of normalizing a term, it is sufficient to consider traversals where we restrict the non-determinism in rule \rulenamet{IVar} such that the visited lambda nodes is necessarily justified be the immediately preceding input variable node.
The intuition is that the non-determinism in those two rules corresponds to branching points in a depth-first exploration of the beta-normal form of the term. Because there is no possible side-effect in lambda terms, it is enough to explore each possible branch of the tree in separate traversals.

\begin{example}[Kierstead terms]
From a game semantic point of view, restricting the opponent moves can be interpreted as restricting the set contexts in which the term can be used, and more particularly the set of terms that can be applied to it.
Like in the affine case, the branching restriction prevents the context from calling the same parameter twice, which eliminates the Kierstead contexts (since the argument $f$ is called twice). But additionally, it also excludes the context $\lambda f \lambda g . f (\lambda f (\lambda x . g (\lambda y . x)))$ where the calls to the two variables $f$ and $g$ are bound by two consecutive lambdas.
\end{example}

\begin{definition}
\label{dfn:branching_traversals}
We define the set of \defname{branching traversals} $\travsetbr$ as the subset of $\travset$ defined by induction with the rules of Table \ref{tab:trav_rules} (substituting $\travsetbr$ for $\travset$ everywhere) with the two input-variable rules replaced by:
\infrule[$\rulefont{IVar}_\branching$]
     {t \cdot n \in \travsetbr
      \andalso n \in\NodeHjByRoot \inter \NodesVar
      \andalso n \enables_k \alpha
      \andalso k \geq 1
     }
     {\Pstr[0.5cm]{t \cdot (n){n} \cdot (l-n,25:k){\alpha}} \in \travsetbr}
\end{definition}


\begin{proposition}[Branching traversals are sufficient for normalization]
\label{prop:branching_traversal_normalizing}
For every traversal $t\in\travset$ there exists $u \in \travsetbr$ such that
$$\pview{u\filter\theroot} = \pview{t\filter\theroot} \ .$$
\end{proposition}
\begin{proof}
\todo{Type my handwritten notes with proof explaining this in more details.}
\end{proof}


\subsection{Arity threshold and normalizing traversals}

In this section, we show that, for the purpose of computing beta normal forms, the value $i$ in rule $\rulename{IVar}$ can be bounded by a computable quantity called the \defname{arity threshold} of a traversal.

When traversing a lambda term, we introduce ghost nodes on-demand each time an eta-expansion is deemed necessary. Recall that our goal is to normalize the term (when it exists) by generating some finite representation of its normal form. We are \emph{not} interested in generating the possibly infinite set of traversals corresponding to the game-semantic denotation of the term. We therefore don't want to eta-expand ad-infinitum: visiting ghost nodes will be useful only if they eventually lead to traversing some structural node of the tree.

Intuitively, after a sufficiently large number of eta-expansions, we are guaranteed to keep on traversing ghost nodes that never materialize back to structural nodes. This section formalizes this intuition by introducing the \defname{arity threshold} as the maximum number of times that is necessary to eta-expand a given subterm with traversal rule $\rulename{IVar}$ in order to compute the set of paths of the beta-normal form of the term. We will show later that such limit is sufficient in order to retrieve the normal form of a lambda term when it exists.

\begin{definition}[Weave]
We call \defname{weave} of a traversal $t$, any sub-sequence of consecutive nodes from $t$,
with even length, starting with a lambda nodes and finishing with a variable node both hereditarily justified by the root, and such that all the occurrences in-between are hereditarily justified by an application node (\ie, not by the root).

From the parity property of traversals, a weave consists of alternations of lambda nodes and variable/application nodes. It is convenient to represent a weave as follows for some $k\geq 1$ where for $j$ ranging from $k$ down to $1$, $\alpha_j$ is a lambda node in $\NodesLmd^\ghostlmd$ and $n_j$ is a variable/application node in $\NodesVar^\ghostvar$:

$$ t = \cdots \underline{\alpha_k}\ n_k\ \alpha_{k-1}\ n_{k-1}\ \cdots\ \alpha_2\ n_2\ \alpha_1\ \underline{n_1} \cdots $$

The first and last occurrences of the weave are underlined; the $2k-2$ nodes occurring in between are those that are not hereditarily justified by the root.

For any occurrence $n$ in $t$ that is hereditarily justified by the root, we call \defname{weave ending at $n$} the sequence of nodes ending at $n$ that constitutes a weave. It can be obtained by taking the longest subsequence of nodes preceding $n$ that are not hereditarily justified by the root.
\end{definition}

\begin{definition}[Traversal arity threshold]
\label{dfn:arity-threshold}
Let $t$ be traversal ending with a variable node hereditarily justified by the root.
Let $\underline{\alpha_k}\ n_k\ \alpha_{k-1}\ n_{k-1}\ \cdots\ \alpha_2\ n_2\ \alpha_1\ \underline{n_1}$ be the weave of $t$ ending at $t^\omega$, (so that $n_1 = t^\omega$) for some $k>0$  where $\alpha_j \in \NodesLmd^\ghostlmd$, and $n_j \in \NodesVar^\ghostvar$ for all $1\leq j\leq k$.

We define the \defname{arity threshold} of $t$ as:
\begin{align*}
\arth(t) &= \max_{q=1..k-1} \left( |n_q| + \sum_{j=1..q-1} (|n_j| - |\alpha_j|) \right)\ .
\end{align*}
\end{definition}

Observe that rule $\rulename{IVar}$ leaves infinitely many choices for the link label: any value greater than $1$. The following result shows that, for calculating paths in the tree representation of the beta-normal form of the term, it suffices to consider link labels that are strictly smaller than the arity threshold.

\begin{remark}[Calculating the arity threshold]
The arity threshold can be rewritten as:
\begin{align*}
\arth(t) &= \max_{q=1..k-1} b_q \ .
\end{align*}
where for $1\leq q\leq k-1$ we define $b_q = \sum_{j=1..q} (|n_j| - |\alpha_j|) + |\alpha_q|$.

The $b_q$'s verify the following induction:
 \begin{align*}
 b_1 &= |n_1| \\
 b_{q+1} &= b_q + |n_{q+1}| - |\alpha_q| & \mbox{$1 \leq q \leq k-2$}
 \end{align*}
In other words: $b_{q} = |n_1| - |\alpha_1| + |n_2| - |\alpha_2| + \ldots + |n_{q-1}| - |\alpha_q| $. So an algorithm to calculate the arity threshold consists in adding and subtracting the arity of the nodes starting from the last occurrence of the traversal and reading backwards until reaching an external lambda node. The maximal value seen during that summation is the arity threshold.
\end{remark}

\begin{property}[Weaving]
\label{prop:weaving}
Let $t \in \travsetbr$ be a branching traversal ending with a variable node hereditarily justified by the root. Let $2k$ be the length of the weave ending at $t^\omega$ for $k\geq1$. Suppose $t_{max} \in \travsetbr$ is a maximal traversal extension of $t$, in which case by rule $\rulename{IVar}_\branching$, the node immediately after $t$ is a lambda node justified by $t^\omega$ with label $i\geq 1$. Then:

\begin{enumerate}
\item If $i>\arth(t)$ then $t^\omega$ is followed in $t_{max}$ by a weave of length $2k$ consisting only of ghost nodes:
$$ t_{max} = \Pstr[0.3cm]{ \cdots
(ak){\underline{\alpha_k}}\ (nk){n_k}\
(ak1)\alpha_{k-1}\ (nk1){n_{k-1}}\
\cdots\
(a2)\alpha_2\ (n2){n_2}\
(a1)\alpha_1\ (n1){\underline{t^\omega}}\
(l1-n1,30:i){\underline{\ghostlmd}}\ (v1-a1,30)\ghostvar\
(l2-n2,30)\ghostlmd\ (v2-a2,30)\ghostvar\
 \cdots\
(lk1-nk1,30)\ghostlmd\ (vk1-ak1,30)\ghostvar\
(lk-nk,30)\ghostlmd\ (vk-ak,30){\underline{\ghostvar}} \cdots } $$

where $\underline{\alpha_k}\ n_k\ \alpha_{k-1}\ n_{k-1}\ \cdots\ \alpha_2\ n_2\ \alpha_1\ \underline{n_1}$ denotes the weave ending at $n_1 = t^\omega$.

\item If $i>\arth(t)$ then \emph{all the nodes} following $t^\omega$ in any traversal extension of $t$ are ghost nodes.
\end{enumerate}
\end{property}
\begin{proof}
(i) Consider the inclusive sub-sequence of nodes from $\alpha$ to $m$. By the alternation property of traversals, it contains $2k$ nodes for some $k\geq1$. Let's write this sequence as a succession of lambda nodes $\alpha_q$ and variable nodes $n_q$, with indices going from $k$ down to $1$. We start indexing from $\alpha$ so that $\alpha_k = \alpha$ and $n_1 = n$.

We show by finite induction on $k\geq1$ that the first $2k$ nodes after $t^\omega$ are successive pairs of ghost lambda and ghost variable nodes justified in order by $n_1, \alpha_1, n_2, \alpha_2, n_3, \ldots, n_k, \alpha_k$ with respective labels $i_1, i_2, i_2, i_3, i_3, \cdots, i_{k-1}, i_{k-1}, i_k$ defined by:
\begin{equation*}
\left\{
    \begin{aligned}
i_1 &= i \\
i_{q+1} &= i_q + |\alpha_q| - |n_q| \hbox{, $1\leq q < k$}.
    \end{aligned}
\right.
\end{equation*}

Or equivalently:
\begin{eqnarray*}
t_{max} &=& \Pstr[0.3cm]{ \cdots
 (alpha){\alpha_k} \ (nk){n_k} \
 (alphakm1){\alpha_{k-1}} \
 \cdots\
 (a2){\alpha_2}\ (n2){n_2}\ (a1){\alpha_1}\ (n1){t^\omega}\
 (l1-n1,30:i_1){\ghostlmd}\
 (t1-a1,40:i_2){\ghostvar} \ (l2-n2,41:i_2){\ghostlmd}\
  (t2-a2,45:i_3){\ghostvar} \cdots
(tkm1-alphakm1,40:i_{k-1}){\ghostvar} \ (lkm1-nk,43:i_{k-1}){\ghostlmd}\
 (tk-alpha,45:i_k){\ghostvar} \cdots }\\
i_{q+1} &=& i + \sum_{j=1..q} (|\alpha_j| - |n_j|)
\end{eqnarray*}

\begin{itemize}
\item Base case $q=1$: Because $t^\omega$ is a ghost variable node hereditarily justified by the root, the only next rule that can be applied is \rulenamet{IVar_\branching}, and by assumption the following node is a ghost lambda node justified by $t^\omega$ with label $i_1 = i$. Then by rule \rulenamet{Lam^\ghostlmd_\branching} the next node is a ghost variable justified by $\alpha$ with label $i_2 = i + |\alpha_1| - |t^\omega|$.

$$ t_{max} = \Pstr[0.3cm]{ \cdots (alpha){\alpha_1}\ (m){t^\omega} (l-m,30:i){\ghostlmd}\ (t-alpha,45:i_2){\ghostvar} \cdots} $$

\item For $1<q\leq k$, by the induction hypothesis we have:
$$
t_{max} = \Pstr[0.3cm]{
   \cdots
  (aq){\alpha_q}\
  (nq){n_q} \
  (aqm1){\alpha_{q-1}}\
  (nqm1){n_{q-1}} \
 \cdots\
 t^\omega\ \cdots
  (lk-nqm1,35:{i_{q-2}})\ghostlmd\
  (tq-aqm1,40:{i_{q-1}})\ghostvar\
   \cdots
}
$$

We then have:
\begin{align*}
i_{q-1} &= i + \sum_{j=1..q-2} (|\alpha_j| - |n_j|)
&\qquad\hbox{(by induction hypothesis)}\\
        &> \arth(t) + \sum_{j=1..q-2} (|\alpha_j| - |n_j|)
&\qquad\hbox{(assumption $i> \arth(t)$)} \\
        &= \max_{r=1..k-1} \left( |n_r| + \sum_{j=1..r-1} (|n_j| - |\alpha_j|) \right)
        + \sum_{j=1..q-2} (|\alpha_j| - |n_j|)
& \qquad\hbox{(definition of $\arth$)} \\
    &\geq |n_q| + \sum_{j=1..q-2} (|n_j| - |\alpha_j|) + \sum_{j=1..q-2} (|\alpha_j| - |n_j|)
&\qquad\hbox{(take $r=q-1$)} \\
    &= |n_q|
\end{align*}


By definition of the weave, $\alpha_{q-1}$ is hereditarily justified by an $@$ node.
Since $i_{q-1} > |n_q|$, by rule \rulenamet{Var_\branching} the next node is necessarily a ghost lambda node justified by $n_q$ with label $i_q$. With rule \rulenamet{Lam^\ghostlmd_\branching} the following node is a ghost variable node $\ghostvar$ justified by $\alpha_q$ and labelled by $i_q = i_{q-1} + |\alpha_{q-1}| -|n_{q-1}|$.
\end{itemize}

(ii) By (i) the next weave following $t$ consists solely of ghost nodes and ends with a ghost variable $\ghostvar$ hereditarily justified by the root. Since ghost nodes have arity $0$, the arity threshold at that point is $0$. Hence, any label value $j$ chosen to extend the traversal at that point will be strictly greater than the arity threshold: thus by (i) the next weave consists solely of ghost nodes.
By repeating the argument this shows that all the nodes following $t$ are necessarily ghost nodes.
\end{proof}


\subsection{Normalization procedure for ULC}

The weaving property \ref{prop:weaving} leads to the following new definition of traversals:
\begin{definition}[Normalizing traversals]
We define the set of \defname{normalizing traversals}, noted $\travsetnorm$ as the set of maximal traversals obtained from the induction rule of Table \ref{tab:trav_rules} where the value $i$ in the rule \rulenamet{IVar} is bounded by the arity of the traversal from Definition \ref{dfn:arity-threshold}:
\infrule[$\rulefont{IVar}_\normalizing$]
     {t \cdot n \in \travsetnorm
     \andalso n \in\NodeHjByRoot \inter \NodesVar^\ghostvar
     \andalso n \enables_k \ghostlmd
     \andalso |n| < k \leq \arth(t)}
     {\Pstr[0.5cm]{t \cdot (n){n} \cdot (l-n,25:k){\ghostlmd}} \in \travsetnorm}
The inductive rules defining $\travsetnorm$ are recapitulated in table \ref{tab:normalizing_trav_rules}.
\end{definition}

\begin{proposition}[Normalizing traversals are sufficient for normalization]
\label{prop:normalizing_traversal_normalizing}
For every traversal $t\in\travset$ there exists $u \in \travsetnorm$ such that
$$\pview{u\filter\theroot} = \pview{t\filter\theroot} \ .$$
\end{proposition}
\begin{proof}
Consequence of Proposition \ref{prop:branching_traversal_normalizing} and Property \ref{prop:weaving}
\end{proof}

\begin{theorem}[Correctness of ULC normalization]
For any untyped lambda term $M$ with normal form $N$ we have:
$$\pview{\travsetnorm(M) \filter\theroot} = \pathset(N) .$$
  \end{theorem}
\begin{proof}[Proof sketch]
It's trivial if $M$ is in beta-normal form.
For any other untyped term $M$ with a normal form, we consider its head-linear reduction sequence.
We know from \cite{danos-head} that $M$'s head-linear reduction sequence leads to some term $M'$ in quasi-head normal form, and that if $M'$ has $n>0$ prime redexes then performing $n$ head reductions produces a head normal form $M''$.
The idea is to show that each of those reduction steps preserves the invariant $\travsetnorm(M)\filter\theroot$.
The head normal form $M''$ is not necessarily normal, however, but we can then recursively apply the same reduction strategy to each argument of the head variable to get to the normal form.
\end{proof}

\begin{FramedTable}
\noindent {\bf PROGRAM - Structural rules}

\infrule[$\rulefont{Root}_\normalizing$]
    {}
    {\theroot \in \travsetnorm}

\infrule[$\rulefont{App}_\normalizing$]
    {t \cdot @ \in \travsetnorm \andalso @ \enables_0 \alpha }
    {\Pstr[0.4cm]{t \cdot (at) @  \cdot (a-at,40:0) \alpha} \in \travsetnorm}

where necessarily $\alpha\in\NodesLmd$, since application nodes can only enable lambda-nodes.

\infrule[$\rulefont{Lam}^@_\normalizing$]
     {t \cdot \alpha \in \travsetnorm
     \andalso \alpha\in\NodesLmd
     \andalso \child(\alpha) \in \Nodes_@ }
     {t \cdot \alpha \cdot \child(\alpha) \in \travsetnorm \andalso \mbox{$\child(\alpha)$ has no pointer}}

\infrule[$\rulefont{Lam}^{\sf var}_\normalizing$]
     {t_1 \cdot \beta \cdot t_2 \cdot \alpha \in \travsetnorm
     \quad \alpha\in\NodesLmd
     \quad \child(\alpha) \in \NodesVar
     \quad \beta \enables_i \child(\alpha), i \geq 1
     \quad \beta \mbox{ visible at } \alpha }
     {\Pstr[0.5cm]{t_1 \cdot (beta)\beta \cdot t_2 \cdot \alpha \cdot (n-beta,30:i){\child(\alpha)
     } \in \travsetnorm}}
where
\begin{compactitem}[-]
\item $\child(\alpha)$ denotes the unique child tree node of lambda-node $\alpha$;
\item $t_1, t_2$ denotes sub-sequences of traversal $t$;
\item ``visible'' means that the node occurs in the P-view $\pview{t}$ of the traversal (\ie, the sub-sequence read from $t$ backwards by following the justification pointer every other  node).
\end{compactitem}

\infrule[$\rulefont{Lam^\ghostlmd_\normalizing}$]
    {
        \Pstr[0.5cm]{ t \cdot(alpha){\alpha} \cdot (m){m} \ldots (gl-m,40:i){\ghostlmd} \in\travsetnorm}
    }
    {
        \Pstr[0.5cm]{ t \cdot(alpha){\alpha} \cdot
        (m){m}
         \ldots
        (gl-m,40:i){\ghostlmd}\cdot (al-alpha,40:{|\alpha| + i - |m|}){\ghostvar}
            \in\travsetnorm}
    }
where necessarily $\alpha \in \NodesLmd^\ghostlmd$ and $m \in\NodesVar^\ghostvar$.
\\

\emph{\bf PROGRAM - Copy-cat rules with on-the-fly eta-expansion}

\infrule[$\rulefont{Var}_\normalizing$]
    {
        \Pstr[0.5cm]{t \cdot m \cdot (alpha){\alpha}
        \ldots (n-alpha,50:i){n} \in \travsetnorm}
        \andalso n \in \NodesVar^\ghostvar \setminus \NodeHjByRoot
        \andalso m \enables_i \beta
        \andalso i>0
    }
    {
        \Pstr[0.5cm]{ t \cdot
        (m){m} \cdot (a){\alpha}  \ldots (n-a,30:i){n}
            \cdot (letai-m,35:i){\beta}
            \in\travsetnorm}
    }
where necessarily $m \in \NodesVar^\ghostvar \union \Nodes_{@}$, $\alpha,\beta \in \NodesLmd^\ghostlmd$, and $\beta = \ghostlmd$ iff $i>|m|$.
\\

\emph{\bf DATA - Input-variable rules}
\infrule[$\rulefont{IVar}_\normalizing$]
     {t \cdot n \in \travsetnorm
      \andalso n \in \NodesVar^\ghostvar \inter \NodeHjByRoot
      \andalso n \enables_i \alpha
      \andalso 1\leq i \leq \arth(t)
     }
     {\Pstr[0.5cm]{t \cdot (n){n} \cdot (l-n,30:i){\alpha}} \in \travsetnorm}
where necessarily $\alpha \in \NodesLmd$ since variable nodes can only enable lambda nodes.

\caption{Normalizing traversal rules for the untyped lambda calculus (ULC).}
\label{tab:normalizing_trav_rules}
\end{FramedTable}


\subsection{ULC Normalization Examples}
In this section we demonstrate on various walk-through examples, how the beta-normal form of a term can be calculated by traversing the term using the normalizing traversals of Table \ref{tab:normalizing_trav_rules}.

\begin{example}[Baby example]
  Take $M = (\lambda x. x x) (\lambda y. y)$. Repeatedly applying the structural traversal rules yields the following traversal:

  $t = \Pstr[0.7cm]{(n0){\lambda }\ (n1){@}\ (n2-n1){\lambda x}\ (n3-n2){x}\ (n4-n1){\lambda y}\ (n5-n4){y}\ (n6-n3){\lambda }\ (n7-n2){x}\ (n8-n1){\lambda y}\ (n9-n8){y}\ (n10-n7){\ghostlmd^1}\ (n11-n6){\ghostvar^1}\ (n12-n5){\ghostlmd^{1}}\ (n13-n4){\ghostvar^2}\ (n14-n3){\ghostlmd^2}\ (n15-n2){\ghostvar^2}\ (n16-n1){\ghostlmd^2}\ (n17-n0){\ghostvar^1}}$

At this point, the last node of $t$ is a variable justified by the root so we could apply rule $\rulename{IVar}$ to get further game-semantic traversals. However, the traversal's arity threshold is $arth(t) = 0$ therefore there is no more \emph{normalizing} traversal to explore: $t$ is a maximal normalizing traversal. The P-view core of the traversal is $\pview{t\filter\theroot} = \Pstr[0.7cm]{(n0){\lambda }\ (n17-n0){\ghostvar^1}}$ thus the beta-normal form of $M$ is, up to $\alpha$-conversion $\lambda y . y$.
\end{example}


\begin{example}[Church increment: ``${\sf add}\ 1$'']
Consider the Church numerals written $k \equiv \lambda s z . s^k z$ for $k\geq0$. We consider a term $M$ representing the increment function that adds $1$ to the input integer: $M k$ reduces to $(k+1)$ for all $k$. Such term can be defined as $M \equiv{\sf add}\ 1$ where
${\sf add} \equiv \lambda x y s z. x\, s (y\, s\, z)$.
Note that $M$ is not beta-normal.
The computation tree $\tau(M)$ of $M$ is:
\begin{tikzpicture}[baseline=(root.base),level distance=5ex,inner ysep=0.5mm,sibling distance=10mm]
\node (root)
{$\lambda$}
child {node{$@$}
    child{node{$\lambda x y s z$}
        child { node{$x$}
            child{node{$\lambda$}{
                child {node {$s$}}}
            }
            child{node{$\lambda$}
                child{node{$y$}
                    child{node{$\lambda$}
                        child{ node {$s$}}
                    }
                    child{node{$\lambda$}
                        child{node{$z$}}}
                }
            }
        }
    }
    child{node{$\lambda s z$}
        child{node{$s$}
            child{node{$\lambda$} child{node{$z$}}}
        }
    }
}
;
\end{tikzpicture}

\begin{itemize}
\item Applying as many deterministic rules as possible gives the following traversal at which point the Opponent must make a choice:

$t_\epsilon = \Pstr[0.7cm]{(n0){\lambda }\ (n1){@}\ (n2-n1){\lambda x y s z}\ (n3-n2){x}\ (n4-n1){\lambda s z}\ (n5-n4){s}\ (n6-n3){\lambda }\ (n7-n2){s}\ (n8-n1){\ghostlmd^3}\ (n9-n0){\ghostvar^2} }$

(For readability we indicate the link label in exponent of the source node when representing traversals.)
The core of $t_\epsilon$ is
$t_\epsilon\filter\theroot = \Pstr[0.7cm]{(n0){\lambda } \cdot (n9-n0){{\ghostvar^2}} }$
and therefore $\pview{t_\epsilon\filter\theroot} =  \lambda \cdot \ghostvar^2$ is a path in  $\betanf{M}$.
This means that $\betanf{M}$ must be of the form $\lambda y s \ldots \cdot y N_1 \ldots \ldots N_q$ for some fresh variable $y$ and $s$ and $q\geq0$.

\item In order to determine what each argument $N_k$ is in the final normal form, we eta-expand by applying rule $\rulename{IVar}$ for each possible argument index $k\geq 1$ and then continue applying the traversal rules.

For $k=1$ we get the traversal:

$t_1 = \Pstr[0.7cm]{(n0){\lambda }\ (n1){@}\ (n2-n1){\lambda x y s z}\ (n3-n2){x}\ (n4-n1){\lambda s z}\
(n5-n4){s}\ (n6-n3){\lambda }\
(n7-n2){s}\ (n8-n1){\ghostlmd^3}\
(n9-n0){\ghostvar^2}
(n10-n9){\ghostlmd^1}
(n11-n8){\ghostvar^1}
(n12-n7){\ghostlmd^1}
(n13-n6){\ghostvar^1}
(n14-n5){\lambda^1}
(n15-n4)z
(n16-n3){\lambda^2}
(n17-n2)y
(n18-n1){\ghostlmd^2}
(n19-n0){\ghostvar^1}
}$

The P-view of the traversal core is
$\pview{t_1\filter\theroot} = \Pstr[0.7cm]{(l){\lambda } \cdot (x-l){\ghostvar^2} \cdot (l1-x){\ghostlmd^1}
\cdot (x2-l){\ghostvar^1}
}$
which means that the normal form is of the form $\lambda y s \ldots \cdot s (y R_1 \ldots R_{q_2}) N_2 \ldots N_q$ for some terms $R_1$, \ldots $R_{q_2}$, and $q,q_2\geq 0$.

\item What should be the value of $q$? In other words, how many more $k$ do we need to look at?  The answer: we need to keep iterating on $k$ until the point where applying the traversal rules will only produce ghost variables and ghost lambda-nodes! Because there is a finite number of nodes in the computation tree, the variable node arities are bounded. Therefore for high enough index $k$, the eta-expansion case from rule \rulenamet{Var} will never be met:
        after applying rule \rulenamet{IVar} on $t_\epsilon$, all subsequent extensions of the traversal will be constructed using repeated applications of rule $\rulename{Lam^\ghostvar}$ or the eta-expanded case of rule $\rulename{Var}$.
     The upper-bound $q$ for $k$ is precisely given by the \emph{arity threshold} of the traversal $t_\epsilon$ as defined in \ref{dfn:arity-threshold}.
     Here we have
     \begin{align*}
     \arth(t_\epsilon)
     = \max \{ & |s^2| , \\
               & |s^1| + (|s^2| - |\lambda|) , \\
               & |x| +  (|s^1| - |\lambda s z|) + (|s^2| - |\lambda|)
               %& |@| + (|x|- |\lambda x y s z |) + (|s^1| - |\lambda s z|) + (|s^2| -|\lambda|) \
               \} \\
    = \max \{   & 0 , \\
                & 1 + (0 - 0) , \\
                & 2 + (1 - 2) + (0 - 0)
                %& 2 + (2 - 4) + (1 - 2) + (0 - 0)
            \} \\
     = 1
     \end{align*}
     Thus we don't need to look at higher value of $k$ at $t_\epsilon$, we thus have:
     $\betanf{M} = \lambda y s \ldots \cdot s (y R_1 \ldots R_{q_2})$.

\item The arity threshold of $t_1$ is $\arth(t_1) = |z| + |y| - |\lambda^2| = 0+2-0 = 2$ hence  $\betanf{M}$ is of the form $\lambda y s \ldots \cdot s (y R_1 R_2)$.

\item Let's eta-expand using rule \rulenamet{IVar} for varying value of child index $1\leq k_2 \leq q_2 = 2$. We first look at the case $k_2 = 1$. The traversal obtained is:

$t_{11} = \Pstr[0.7cm]{
(n0){\lambda }\
(n1){@}\ (n2-n1){\lambda x y s z}\ (n3-n2){x}\ (n4-n1){\lambda s z}\ (n5-n4){s}\ (n6-n3){\lambda }\ (n7-n2){s}\ (n8-n1){\ghostlmd^3}\ (n9-n0){\ghostvar^2}
(n10-n9){\ghostlmd^1}
(n11-n8){\ghostvar^1}
(n12-n7){\ghostlmd^1}
(n13-n6){\ghostvar^1}
(n14-n5){\lambda^1}
(n15-n4)z
(n16-n3){\lambda^2}
(n17-n2)y
(n18-n1){\ghostlmd^2}
(n19-n0){\ghostvar^1}
(n20-n19){\ghostlmd^1}
(n21-n18){\ghostvar^1}
(n22-n17,60:1)\lambda % points to non-ghost node
(n23-n2,45:3) s
(n24-n1,45:3) {\ghostlmd^3}
(n25-n0,48:2) {\ghostvar^2}
}$

Thus $\pview{t_{11} \filter\theroot} =
\Pstr[0.7cm]{
(n0){\lambda }\
 (n9-n0){\ghostvar^2}
 (n10-n9){\ghostlmd^1}
(n19-n0){\ghostvar^1}
(n20-n19){\ghostlmd^1}
(n25-n0,48:2){\ghostvar^2}
}$

Hence $\betanf{M}$ is of the form $\lambda y s \ldots \cdot s\ (y\ s\ R_2)$.

\item Now let's extend $t_1$ with \rulenamet{IVar^\lambda} for $k_2 = 2$. We get the traversal:

$t_{12} = \Pstr[0.7cm]{
(n0){\lambda }\
(n1){@}\ (n2-n1){\lambda x y s z}\
(n3-n2){x}\ (n4-n1){\lambda s z}\
(n5-n4){s}\
(n6-n3){\lambda }\
(n7-n2){s}\
(n8-n1){\ghostlmd^3}\
(n9-n0) {\ghostvar^2}
(n10-n9) {\ghostlmd^1}
(n11-n8){\ghostvar^1}
(n12-n7){\ghostlmd^1}
(n13-n6){\ghostvar^1}
(n14-n5)\lambda^1
(n15-n4)z
(n16-n3){\lambda^2}
(n17-n2)y
(n18-n1){\ghostlmd^2}
(n19-n0){\ghostvar^1}
(n20-n19){\ghostlmd^2} %%%%%%%%
(n21-n18){\ghostvar^2}
(n22-n17,60:2)\lambda % points to non-ghost node
(n23-n2,45:4) z
(n24-n1,45:4) {\ghostlmd^4}
(n25-n0,48:3) {\ghostvar^3}
}$

Thus $\pview{t_{12} \filter\theroot} =
\Pstr[0.7cm]{
(n0){\lambda }\
 (n9-n0){{\ghostvar^2}}
 (n10-n9){\ghostlmd^1}
(n19-n0){\ghostvar^1}
(n20-n19){\ghostlmd^2}
(n25-n0,48:3) {\ghostvar^3}
}$

Hence $\betanf{M} = \lambda y s z \cdot s\ (y\ s\ z)$.
\end{itemize}
\end{example}

\begin{example}[``Missing operand'' example by Neil Jones]
This small example illustrates how on-the-fly eta-expansion helps resolve the ``missing argument'' problem faced when using the traversal rules of STLC. Take $M = (\lambda u . u\ (y_1\ u)) (\lambda v . v\ y_2)$.

The computation tree is:
\begin{tikzpicture}[baseline=(root.base),level distance=5ex,inner ysep=0.5mm,sibling distance=10mm]
\node (root)
{$\lambda$}
child {node{$@$}
        child{node{$\lambda u $}
           child {node {$u$}
              child {node {$\lambda$}
                  child {node {$y_1$}
                      child {node {$\lambda$}
                          child {node {$u$}
                          }
                      }
                  }
              }
           }
        }
        child{node{$\lambda v$}
            child{node{$v$}
                child{node{$\lambda$}
                    child{ node {$y_2$}}
                }
            }
        }
    }
;
\end{tikzpicture}

The only two maximal normalizing traversals are:
\begin{itemize}

\item $t_1 = \Pstr[0.7cm]{(n0){\lambda }\ (n1){@}\ (n2-n1){\lambda u}\ (n3-n2){u}\ (n4-n1){\lambda v}\ (n5-n4){v}\ (n6-n3){\lambda }\ (n7-n0){y_1}\ (n8-n7){\lambda }\ (n9-n2){u}\ (n10-n1){\lambda v}\ (n11-n10){v}\ (n12-n9){{\ghostlmd^{1}}}\ (n13-n8){{\ghostvar^{1}}}\ (n14-n13){{\ghostlmd^{1}}}\ (n15-n12){{\ghostvar^{1}}}\ (n16-n11){\lambda }\ (n17-n0,35){y_2}\ }$
\item $t_2 = \Pstr[0.7cm]{(n0){\lambda }\ (n1){@}\ (n2-n1){\lambda u}\ (n3-n2){u}\ (n4-n1){\lambda v}\ (n5-n4){v}\ (n6-n3){\lambda }\ (n7-n0){y_1}\ (n8-n7){{\ghostlmd^{2}}}\ (n9-n6){{\ghostvar^{1}}}\ (n10-n5){\lambda }\ (n11-n0){y_2}\ }$
\end{itemize}
Using the STLC traversals, one can traverse $t_1$ all the way to variable $v$ at which point we get stuck due to the lack of operand applied to the last occurrence of $u$ (the one at the bottom of the left branch in the tree). Using ULC rule, that operands gets created on-the-fly through eta-expansion and is represented by ghost lambda node $\ghostlmd^1$.

The two core P-views give the two paths in the beta-normal form of $M$:
\begin{itemize}
\item $\pview{t_1\filter\theroot} = \Pstr[0.7cm]{(n0){\lambda }\ (n1-n0){y_1}\ (n2-n1){\lambda }\ (n3-n2){{\ghostvar^{1}}}\ (n4-n3){{\ghostlmd^{1}}}\ (n5-n0){y2}\ }$
\item $\pview{t_2\filter\theroot} = \Pstr[0.7cm]{(n0){\lambda }\ (n1-n0){y_1}\ (n2-n1){{\ghostlmd^{2}}}\ (n3-n0){y_2}\ }$
\end{itemize}
Thus: $\betanf{M} = \lambda x . y_1 (\lambda  z.z\ y_2) y_2$.
\end{example}


\begin{example}[Neil Jones' ``$varity\ 2$'' example]

Take $M = varity\ two$ where
\begin{align*}
  varity &\equiv \lambda t.t (\lambda n a x . n (\lambda s z . a s (x s z))) (\lambda a. a) (\lambda z_0 . z_0) \\
  two &\equiv \lambda s_2 z_2 . s_2 (s_2\ z_2)
\end{align*}

In order to compute the set of normalizing traversals, we apply the traversal rules inductively until an input variable is reached, at which point we eta-expand with rule \rulenamet{IVar} for all possible values of $k$ ranging from $1$ to the arity threshold of the traversal. The first traversal obtained is denoted $t_\epsilon$, and for every sequence of integer $s \in \nat^*$,  the traversal $t_{s \cdot k}$ represents the maximal traversal obtained after extending $t_s$ using one application of the eta-expanded subcase of rule \rulenamet{IVar} with link-label $k \in \nat$.
This process yields the set of traversals $\{t_\epsilon, t_{11}, t_{12}, t_{121}, t_{122} \}$ represented in the following pages.

\begin{landscape}
\resizebox{1.1\hsize}{!}{$
t_\epsilon = \Pstr[0.7cm]{(n0){\lambda }\ (n1){@}\ (n2-n1){\lambda t}\ (n3-n2){t}\ (n4-n1){\lambda s_2 z_2}\ (n5-n4){s_2}\ (n6-n3){\lambda n a x}\ (n7-n6){n}\ (n8-n5){\lambda }\ (n9-n4){s_2}\ (n10-n3){\lambda n a x}\ (n11-n10){n}\ (n12-n9){\lambda }\ (n13-n4){z_2}\ (n14-n3){\lambda a}\ (n15-n14){a}\ (n16-n13){{\ghostlmd^{1}}}\ (n17-n12){{\ghostvar^{1}}}\ (n18-n11){\lambda s z}\ (n19-n10){a}\ (n20-n9){{\ghostlmd^{2}}}\ (n21-n8){{\ghostvar^{1}}}\ (n22-n7){\lambda s z}\ (n23-n6){a}\ (n24-n5){{\ghostlmd^{2}}}\ (n25-n4){{\ghostvar^{3}}}\
(n26-n3){\lambda z0}\ (n27-n26){z0}\ (n28-n25){{\ghostlmd^{1}}}\ (n29-n24){{\ghostvar^{1}}}\
(n30-n23){\lambda }\ (n31-n22){s}\ (n32-n21){{\ghostlmd^{1}}}\ (n33-n20){{\ghostvar^{1}}}\ (n34-n19){\lambda }\ (n35-n18){s}\ (n36-n17){{\ghostlmd^{1}}}\ (n37-n16){{\ghostvar^{1}}}\ (n38-n15){{\ghostlmd^{1}}}\ (n39-n14){{\ghostvar^{2}}}\ (n40-n13){{\ghostlmd^{2}}}\ (n41-n12){{\ghostvar^{2}}}\ (n42-n11){{\ghostlmd^{2}}}\ (n43-n10){{\ghostvar^{4}}}\ (n44-n9){{\ghostlmd^{4}}}\ (n45-n8){{\ghostvar^{3}}}\ (n46-n7){{\ghostlmd^{3}}}\ (n47-n6){{\ghostvar^{5}}}\ (n48-n5){{\ghostlmd^{5}}}\ (n49-n4){{\ghostvar^{6}}}\ (n50-n3){{\ghostlmd^{6}}}\ (n51-n2){{\ghostvar^{4}}}\ (n52-n1){{\ghostlmd^{4}}}\ (n53-n0){{\ghostvar^{3}}}}$}

\resizebox{1.1\hsize}{!}{$t_{1} = \Pstr[0.7cm]{(n0){\lambda }\ (n1){@}\ (n2-n1){\lambda t}\ (n3-n2){t}\ (n4-n1){\lambda s_2 z_2}\ (n5-n4){s_2}\ (n6-n3){\lambda n a x}\ (n7-n6){n}\ (n8-n5){\lambda }\ (n9-n4){s_2}\ (n10-n3){\lambda n a x}\ (n11-n10){n}\ (n12-n9){\lambda }\ (n13-n4){z_2}\ (n14-n3){\lambda a}\ (n15-n14){a}\ (n16-n13){{\ghostlmd^{1}}}\ (n17-n12){{\ghostvar^{1}}}\ (n18-n11){\lambda s z}\ (n19-n10){a}\ (n20-n9){{\ghostlmd^{2}}}\ (n21-n8){{\ghostvar^{1}}}\ (n22-n7){\lambda s z}\ (n23-n6){a}\ (n24-n5){{\ghostlmd^{2}}}\ (n25-n4){{\ghostvar^{3}}}\ (n26-n3){\lambda z0}\ (n27-n26){z0}\ (n28-n25){{\ghostlmd^{1}}}\ (n29-n24){{\ghostvar^{1}}}\ (n30-n23){\lambda }\ (n31-n22){s}\ (n32-n21){{\ghostlmd^{1}}}\ (n33-n20){{\ghostvar^{1}}}\ (n34-n19){\lambda }\ (n35-n18){s}\ (n36-n17){{\ghostlmd^{1}}}\ (n37-n16){{\ghostvar^{1}}}\ (n38-n15){{\ghostlmd^{1}}}\ (n39-n14){{\ghostvar^{2}}}\ (n40-n13){{\ghostlmd^{2}}}\ (n41-n12){{\ghostvar^{2}}}\ (n42-n11){{\ghostlmd^{2}}}\ (n43-n10){{\ghostvar^{4}}}\ (n44-n9){{\ghostlmd^{4}}}\ (n45-n8){{\ghostvar^{3}}}\ (n46-n7){{\ghostlmd^{3}}}\ (n47-n6){{\ghostvar^{5}}}\ (n48-n5){{\ghostlmd^{5}}}\ (n49-n4){{\ghostvar^{6}}}\ (n50-n3){{\ghostlmd^{6}}}\ (n51-n2){{\ghostvar^{4}}}\ (n52-n1){{\ghostlmd^{4}}}\ (n53-n0){{\ghostvar^{3}}}\ (n54-n53){{\ghostlmd^{1}}}\ (n55-n52){{\ghostvar^{1}}}\ (n56-n51){{\ghostlmd^{1}}}\ (n57-n50){{\ghostvar^{1}}}\ (n58-n49){{\ghostlmd^{1}}}\ (n59-n48){{\ghostvar^{1}}}\ (n60-n47){{\ghostlmd^{1}}}\ (n61-n46){{\ghostvar^{1}}}\ (n62-n45){{\ghostlmd^{1}}}\ (n63-n44){{\ghostvar^{1}}}\ (n64-n43){{\ghostlmd^{1}}}\ (n65-n42){{\ghostvar^{1}}}\ (n66-n41){{\ghostlmd^{1}}}\ (n67-n40){{\ghostvar^{1}}}\ (n68-n39){{\ghostlmd^{1}}}\ (n69-n38){{\ghostvar^{1}}}\ (n70-n37){{\ghostlmd^{1}}}\ (n71-n36){{\ghostvar^{1}}}\ (n72-n35){{\ghostlmd^{1}}}\ (n73-n34){{\ghostvar^{1}}}\ (n74-n33){{\ghostlmd^{1}}}\ (n75-n32){{\ghostvar^{1}}}\ (n76-n31){{\ghostlmd^{1}}}\ (n77-n30){{\ghostvar^{1}}}\ (n78-n29){{\ghostlmd^{1}}}\ (n79-n28){{\ghostvar^{1}}}\ (n80-n27){{\ghostlmd^{1}}}\ (n81-n26){{\ghostvar^{2}}}\ (n82-n25){{\ghostlmd^{2}}}\ (n83-n24){{\ghostvar^{2}}}\ (n84-n23){\lambda }\ (n85-n6){x}\ (n86-n5){{\ghostlmd^{3}}}\ (n87-n4){{\ghostvar^{4}}}\ (n88-n3){{\ghostlmd^{4}}}\ (n89-n2){{\ghostvar^{2}}}\ (n90-n1){{\ghostlmd^{2}}}\ (n91-n0){{\ghostvar^{1}}}}$}


\resizebox{1.1\hsize}{!}{$t_{11} = \Pstr[0.7cm]{(n0){\lambda }\ (n1){@}\ (n2-n1){\lambda t}\ (n3-n2){t}\ (n4-n1){\lambda s_2 z_2}\ (n5-n4){s_2}\ (n6-n3){\lambda n a x}\ (n7-n6){n}\ (n8-n5){\lambda }\ (n9-n4){s_2}\ (n10-n3){\lambda n a x}\ (n11-n10){n}\ (n12-n9){\lambda }\ (n13-n4){z_2}\ (n14-n3){\lambda a}\ (n15-n14){a}\ (n16-n13){{\ghostlmd^{1}}}\ (n17-n12){{\ghostvar^{1}}}\ (n18-n11){\lambda s z}\ (n19-n10){a}\ (n20-n9){{\ghostlmd^{2}}}\ (n21-n8){{\ghostvar^{1}}}\ (n22-n7){\lambda s z}\ (n23-n6){a}\ (n24-n5){{\ghostlmd^{2}}}\ (n25-n4){{\ghostvar^{3}}}\ (n26-n3){\lambda z0}\ (n27-n26){z0}\ (n28-n25){{\ghostlmd^{1}}}\ (n29-n24){{\ghostvar^{1}}}\ (n30-n23){\lambda }\ (n31-n22){s}\ (n32-n21){{\ghostlmd^{1}}}\ (n33-n20){{\ghostvar^{1}}}\ (n34-n19){\lambda }\ (n35-n18){s}\ (n36-n17){{\ghostlmd^{1}}}\ (n37-n16){{\ghostvar^{1}}}\ (n38-n15){{\ghostlmd^{1}}}\ (n39-n14){{\ghostvar^{2}}}\ (n40-n13){{\ghostlmd^{2}}}\ (n41-n12){{\ghostvar^{2}}}\ (n42-n11){{\ghostlmd^{2}}}\ (n43-n10){{\ghostvar^{4}}}\ (n44-n9){{\ghostlmd^{4}}}\ (n45-n8){{\ghostvar^{3}}}\ (n46-n7){{\ghostlmd^{3}}}\ (n47-n6){{\ghostvar^{5}}}\ (n48-n5){{\ghostlmd^{5}}}\ (n49-n4){{\ghostvar^{6}}}\ (n50-n3){{\ghostlmd^{6}}}\ (n51-n2){{\ghostvar^{4}}}\ (n52-n1){{\ghostlmd^{4}}}\ (n53-n0){{\ghostvar^{3}}}\ (n54-n53){{\ghostlmd^{1}}}\ (n55-n52){{\ghostvar^{1}}}\ (n56-n51){{\ghostlmd^{1}}}\ (n57-n50){{\ghostvar^{1}}}\ (n58-n49){{\ghostlmd^{1}}}\ (n59-n48){{\ghostvar^{1}}}\ (n60-n47){{\ghostlmd^{1}}}\ (n61-n46){{\ghostvar^{1}}}\ (n62-n45){{\ghostlmd^{1}}}\ (n63-n44){{\ghostvar^{1}}}\ (n64-n43){{\ghostlmd^{1}}}\ (n65-n42){{\ghostvar^{1}}}\ (n66-n41){{\ghostlmd^{1}}}\ (n67-n40){{\ghostvar^{1}}}\ (n68-n39){{\ghostlmd^{1}}}\ (n69-n38){{\ghostvar^{1}}}\ (n70-n37){{\ghostlmd^{1}}}\ (n71-n36){{\ghostvar^{1}}}\ (n72-n35){{\ghostlmd^{1}}}\ (n73-n34){{\ghostvar^{1}}}\ (n74-n33){{\ghostlmd^{1}}}\ (n75-n32){{\ghostvar^{1}}}\ (n76-n31){{\ghostlmd^{1}}}\ (n77-n30){{\ghostvar^{1}}}\ (n78-n29){{\ghostlmd^{1}}}\ (n79-n28){{\ghostvar^{1}}}\ (n80-n27){{\ghostlmd^{1}}}\ (n81-n26){{\ghostvar^{2}}}\ (n82-n25){{\ghostlmd^{2}}}\ (n83-n24){{\ghostvar^{2}}}\ (n84-n23){\lambda }\ (n85-n6){x}\ (n86-n5){{\ghostlmd^{3}}}\ (n87-n4){{\ghostvar^{4}}}\ (n88-n3){{\ghostlmd^{4}}}\ (n89-n2){{\ghostvar^{2}}}\ (n90-n1){{\ghostlmd^{2}}}\ (n91-n0){{\ghostvar^{1}}}\ (n92-n91){{\ghostlmd^{1}}}\ (n93-n90){{\ghostvar^{1}}}\ (n94-n89){{\ghostlmd^{1}}}\ (n95-n88){{\ghostvar^{1}}}\ (n96-n87){{\ghostlmd^{1}}}\ (n97-n86){{\ghostvar^{1}}}\ (n98-n85){\lambda }\ (n99-n22){s}\ (n100-n21){{\ghostlmd^{1}}}\ (n101-n20){{\ghostvar^{1}}}\ (n102-n19){\lambda }\ (n103-n18){s}\ (n104-n17){{\ghostlmd^{1}}}\ (n105-n16){{\ghostvar^{1}}}\ (n106-n15){{\ghostlmd^{1}}}\ (n107-n14){{\ghostvar^{2}}}\ (n108-n13){{\ghostlmd^{2}}}\ (n109-n12){{\ghostvar^{2}}}\ (n110-n11){{\ghostlmd^{2}}}\ (n111-n10){{\ghostvar^{4}}}\ (n112-n9){{\ghostlmd^{4}}}\ (n113-n8){{\ghostvar^{3}}}\ (n114-n7){{\ghostlmd^{3}}}\ (n115-n6){{\ghostvar^{5}}}\ (n116-n5){{\ghostlmd^{5}}}\ (n117-n4){{\ghostvar^{6}}}\ (n118-n3){{\ghostlmd^{6}}}\ (n119-n2){{\ghostvar^{4}}}\ (n120-n1){{\ghostlmd^{4}}}\ (n121-n0){{\ghostvar^{3}}}}$}

\resizebox{1.1\hsize}{!}{$t_{12} = \Pstr[0.7cm]{(n0){\lambda }\ (n1){@}\ (n2-n1){\lambda t}\ (n3-n2){t}\ (n4-n1){\lambda s_2 z_2}\ (n5-n4){s_2}\ (n6-n3){\lambda n a x}\ (n7-n6){n}\ (n8-n5){\lambda }\ (n9-n4){s_2}\ (n10-n3){\lambda n a x}\ (n11-n10){n}\ (n12-n9){\lambda }\ (n13-n4){z_2}\ (n14-n3){\lambda a}\ (n15-n14){a}\ (n16-n13){{\ghostlmd^{1}}}\ (n17-n12){{\ghostvar^{1}}}\ (n18-n11){\lambda s z}\ (n19-n10){a}\ (n20-n9){{\ghostlmd^{2}}}\ (n21-n8){{\ghostvar^{1}}}\ (n22-n7){\lambda s z}\ (n23-n6){a}\ (n24-n5){{\ghostlmd^{2}}}\ (n25-n4){{\ghostvar^{3}}}\ (n26-n3){\lambda z0}\ (n27-n26){z0}\ (n28-n25){{\ghostlmd^{1}}}\ (n29-n24){{\ghostvar^{1}}}\ (n30-n23){\lambda }\ (n31-n22){s}\ (n32-n21){{\ghostlmd^{1}}}\ (n33-n20){{\ghostvar^{1}}}\ (n34-n19){\lambda }\ (n35-n18){s}\ (n36-n17){{\ghostlmd^{1}}}\ (n37-n16){{\ghostvar^{1}}}\ (n38-n15){{\ghostlmd^{1}}}\ (n39-n14){{\ghostvar^{2}}}\ (n40-n13){{\ghostlmd^{2}}}\ (n41-n12){{\ghostvar^{2}}}\ (n42-n11){{\ghostlmd^{2}}}\ (n43-n10){{\ghostvar^{4}}}\ (n44-n9){{\ghostlmd^{4}}}\ (n45-n8){{\ghostvar^{3}}}\ (n46-n7){{\ghostlmd^{3}}}\ (n47-n6){{\ghostvar^{5}}}\ (n48-n5){{\ghostlmd^{5}}}\ (n49-n4){{\ghostvar^{6}}}\ (n50-n3){{\ghostlmd^{6}}}\ (n51-n2){{\ghostvar^{4}}}\ (n52-n1){{\ghostlmd^{4}}}\ (n53-n0){{\ghostvar^{3}}}\ (n54-n53){{\ghostlmd^{1}}}\ (n55-n52){{\ghostvar^{1}}}\ (n56-n51){{\ghostlmd^{1}}}\ (n57-n50){{\ghostvar^{1}}}\ (n58-n49){{\ghostlmd^{1}}}\ (n59-n48){{\ghostvar^{1}}}\ (n60-n47){{\ghostlmd^{1}}}\ (n61-n46){{\ghostvar^{1}}}\ (n62-n45){{\ghostlmd^{1}}}\ (n63-n44){{\ghostvar^{1}}}\ (n64-n43){{\ghostlmd^{1}}}\ (n65-n42){{\ghostvar^{1}}}\ (n66-n41){{\ghostlmd^{1}}}\ (n67-n40){{\ghostvar^{1}}}\ (n68-n39){{\ghostlmd^{1}}}\ (n69-n38){{\ghostvar^{1}}}\ (n70-n37){{\ghostlmd^{1}}}\ (n71-n36){{\ghostvar^{1}}}\ (n72-n35){{\ghostlmd^{1}}}\ (n73-n34){{\ghostvar^{1}}}\ (n74-n33){{\ghostlmd^{1}}}\ (n75-n32){{\ghostvar^{1}}}\ (n76-n31){{\ghostlmd^{1}}}\ (n77-n30){{\ghostvar^{1}}}\ (n78-n29){{\ghostlmd^{1}}}\ (n79-n28){{\ghostvar^{1}}}\ (n80-n27){{\ghostlmd^{1}}}\ (n81-n26){{\ghostvar^{2}}}\ (n82-n25){{\ghostlmd^{2}}}\ (n83-n24){{\ghostvar^{2}}}\ (n84-n23){\lambda }\ (n85-n6){x}\ (n86-n5){{\ghostlmd^{3}}}\ (n87-n4){{\ghostvar^{4}}}\ (n88-n3){{\ghostlmd^{4}}}\ (n89-n2){{\ghostvar^{2}}}\ (n90-n1){{\ghostlmd^{2}}}\ (n91-n0){{\ghostvar^{1}}}\ (n92-n91){{\ghostlmd^{2}}}\ (n93-n90){{\ghostvar^{2}}}\ (n94-n89){{\ghostlmd^{2}}}\ (n95-n88){{\ghostvar^{2}}}\ (n96-n87){{\ghostlmd^{2}}}\ (n97-n86){{\ghostvar^{2}}}\ (n98-n85){\lambda }\ (n99-n22){z}\ (n100-n21){{\ghostlmd^{2}}}\ (n101-n20){{\ghostvar^{2}}}\ (n102-n19){\lambda }\ (n103-n10){x}\ (n104-n9){{\ghostlmd^{3}}}\ (n105-n8){{\ghostvar^{2}}}\ (n106-n7){{\ghostlmd^{2}}}\ (n107-n6){{\ghostvar^{4}}}\ (n108-n5){{\ghostlmd^{4}}}\ (n109-n4){{\ghostvar^{5}}}\ (n110-n3){{\ghostlmd^{5}}}\ (n111-n2){{\ghostvar^{3}}}\ (n112-n1){{\ghostlmd^{3}}}\ (n113-n0){{\ghostvar^{2}}}}$}


\resizebox{1.1\hsize}{!}{$t_{121} =
\Pstr[0.7cm]{(n0){\lambda }\ (n1){@}\ (n2-n1){\lambda t}\ (n3-n2){t}\ (n4-n1){\lambda s_2 z_2}\ (n5-n4){s_2}\ (n6-n3){\lambda n a x}\ (n7-n6){n}\ (n8-n5){\lambda }\ (n9-n4){s_2}\ (n10-n3){\lambda n a x}\ (n11-n10){n}\ (n12-n9){\lambda }\ (n13-n4){z_2}\ (n14-n3){\lambda a}\ (n15-n14){a}\ (n16-n13){{\ghostlmd^{1}}}\ (n17-n12){{\ghostvar^{1}}}\ (n18-n11){\lambda s z}\ (n19-n10){a}\ (n20-n9){{\ghostlmd^{2}}}\ (n21-n8){{\ghostvar^{1}}}\ (n22-n7){\lambda s z}\ (n23-n6){a}\ (n24-n5){{\ghostlmd^{2}}}\ (n25-n4){{\ghostvar^{3}}}\ (n26-n3){\lambda z0}\ (n27-n26){z0}\ (n28-n25){{\ghostlmd^{1}}}\ (n29-n24){{\ghostvar^{1}}}\ (n30-n23){\lambda }\ (n31-n22){s}\ (n32-n21){{\ghostlmd^{1}}}\ (n33-n20){{\ghostvar^{1}}}\ (n34-n19){\lambda }\ (n35-n18){s}\ (n36-n17){{\ghostlmd^{1}}}\ (n37-n16){{\ghostvar^{1}}}\ (n38-n15){{\ghostlmd^{1}}}\ (n39-n14){{\ghostvar^{2}}}\ (n40-n13){{\ghostlmd^{2}}}\ (n41-n12){{\ghostvar^{2}}}\ (n42-n11){{\ghostlmd^{2}}}\ (n43-n10){{\ghostvar^{4}}}\ (n44-n9){{\ghostlmd^{4}}}\ (n45-n8){{\ghostvar^{3}}}\ (n46-n7){{\ghostlmd^{3}}}\ (n47-n6){{\ghostvar^{5}}}\ (n48-n5){{\ghostlmd^{5}}}\ (n49-n4){{\ghostvar^{6}}}\ (n50-n3){{\ghostlmd^{6}}}\ (n51-n2){{\ghostvar^{4}}}\ (n52-n1){{\ghostlmd^{4}}}\ (n53-n0){{\ghostvar^{3}}}\ (n54-n53){{\ghostlmd^{1}}}\ (n55-n52){{\ghostvar^{1}}}\ (n56-n51){{\ghostlmd^{1}}}\ (n57-n50){{\ghostvar^{1}}}\ (n58-n49){{\ghostlmd^{1}}}\ (n59-n48){{\ghostvar^{1}}}\ (n60-n47){{\ghostlmd^{1}}}\ (n61-n46){{\ghostvar^{1}}}\ (n62-n45){{\ghostlmd^{1}}}\ (n63-n44){{\ghostvar^{1}}}\ (n64-n43){{\ghostlmd^{1}}}\ (n65-n42){{\ghostvar^{1}}}\ (n66-n41){{\ghostlmd^{1}}}\ (n67-n40){{\ghostvar^{1}}}\ (n68-n39){{\ghostlmd^{1}}}\ (n69-n38){{\ghostvar^{1}}}\ (n70-n37){{\ghostlmd^{1}}}\ (n71-n36){{\ghostvar^{1}}}\ (n72-n35){{\ghostlmd^{1}}}\ (n73-n34){{\ghostvar^{1}}}\ (n74-n33){{\ghostlmd^{1}}}\ (n75-n32){{\ghostvar^{1}}}\ (n76-n31){{\ghostlmd^{1}}}\ (n77-n30){{\ghostvar^{1}}}\ (n78-n29){{\ghostlmd^{1}}}\ (n79-n28){{\ghostvar^{1}}}\ (n80-n27){{\ghostlmd^{1}}}\ (n81-n26){{\ghostvar^{2}}}\ (n82-n25){{\ghostlmd^{2}}}\ (n83-n24){{\ghostvar^{2}}}\ (n84-n23){\lambda }\ (n85-n6){x}\ (n86-n5){{\ghostlmd^{3}}}\ (n87-n4){{\ghostvar^{4}}}\ (n88-n3){{\ghostlmd^{4}}}\ (n89-n2){{\ghostvar^{2}}}\ (n90-n1){{\ghostlmd^{2}}}\ (n91-n0){{\ghostvar^{1}}}\ (n92-n91){{\ghostlmd^{2}}}\ (n93-n90){{\ghostvar^{2}}}\ (n94-n89){{\ghostlmd^{2}}}\ (n95-n88){{\ghostvar^{2}}}\ (n96-n87){{\ghostlmd^{2}}}\ (n97-n86){{\ghostvar^{2}}}\ (n98-n85){\lambda }\ (n99-n22){z}\ (n100-n21){{\ghostlmd^{2}}}\ (n101-n20){{\ghostvar^{2}}}\ (n102-n19){\lambda }\ (n103-n10){x}\ (n104-n9){{\ghostlmd^{3}}}\ (n105-n8){{\ghostvar^{2}}}\ (n106-n7){{\ghostlmd^{2}}}\ (n107-n6){{\ghostvar^{4}}}\ (n108-n5){{\ghostlmd^{4}}}\ (n109-n4){{\ghostvar^{5}}}\ (n110-n3){{\ghostlmd^{5}}}\ (n111-n2){{\ghostvar^{3}}}\ (n112-n1){{\ghostlmd^{3}}}\ (n113-n0){{\ghostvar^{2}}}\ (n114-n113){{\ghostlmd^{1}}}\ (n115-n112){{\ghostvar^{1}}}\ (n116-n111){{\ghostlmd^{1}}}\ (n117-n110){{\ghostvar^{1}}}\ (n118-n109){{\ghostlmd^{1}}}\ (n119-n108){{\ghostvar^{1}}}\ (n120-n107){{\ghostlmd^{1}}}\ (n121-n106){{\ghostvar^{1}}}\ (n122-n105){{\ghostlmd^{1}}}\ (n123-n104){{\ghostvar^{1}}}\ (n124-n103){\lambda }\ (n125-n18){s}\ (n126-n17){{\ghostlmd^{1}}}\ (n127-n16){{\ghostvar^{1}}}\ (n128-n15){{\ghostlmd^{1}}}\ (n129-n14){{\ghostvar^{2}}}\ (n130-n13){{\ghostlmd^{2}}}\ (n131-n12){{\ghostvar^{2}}}\ (n132-n11){{\ghostlmd^{2}}}\ (n133-n10){{\ghostvar^{4}}}\ (n134-n9){{\ghostlmd^{4}}}\ (n135-n8){{\ghostvar^{3}}}\ (n136-n7){{\ghostlmd^{3}}}\ (n137-n6){{\ghostvar^{5}}}\ (n138-n5){{\ghostlmd^{5}}}\ (n139-n4){{\ghostvar^{6}}}\ (n140-n3){{\ghostlmd^{6}}}\ (n141-n2){{\ghostvar^{4}}}\ (n142-n1){{\ghostlmd^{4}}}\ (n143-n0){{\ghostvar^{3}}}}$}

\resizebox{1.1\hsize}{!}{$t_{122} =
\Pstr[0.7cm]{(n0){\lambda }\ (n1){@}\ (n2-n1){\lambda t}\ (n3-n2){t}\ (n4-n1){\lambda s_2 z_2}\ (n5-n4){s_2}\ (n6-n3){\lambda n a x}\ (n7-n6){n}\ (n8-n5){\lambda }\ (n9-n4){s_2}\ (n10-n3){\lambda n a x}\ (n11-n10){n}\ (n12-n9){\lambda }\ (n13-n4){z_2}\ (n14-n3){\lambda a}\ (n15-n14){a}\ (n16-n13){{\ghostlmd^{1}}}\ (n17-n12){{\ghostvar^{1}}}\ (n18-n11){\lambda s z}\ (n19-n10){a}\ (n20-n9){{\ghostlmd^{2}}}\ (n21-n8){{\ghostvar^{1}}}\ (n22-n7){\lambda s z}\ (n23-n6){a}\ (n24-n5){{\ghostlmd^{2}}}\ (n25-n4){{\ghostvar^{3}}}\ (n26-n3){\lambda z0}\ (n27-n26){z0}\ (n28-n25){{\ghostlmd^{1}}}\ (n29-n24){{\ghostvar^{1}}}\ (n30-n23){\lambda }\ (n31-n22){s}\ (n32-n21){{\ghostlmd^{1}}}\ (n33-n20){{\ghostvar^{1}}}\ (n34-n19){\lambda }\ (n35-n18){s}\ (n36-n17){{\ghostlmd^{1}}}\ (n37-n16){{\ghostvar^{1}}}\ (n38-n15){{\ghostlmd^{1}}}\ (n39-n14){{\ghostvar^{2}}}\ (n40-n13){{\ghostlmd^{2}}}\ (n41-n12){{\ghostvar^{2}}}\ (n42-n11){{\ghostlmd^{2}}}\ (n43-n10){{\ghostvar^{4}}}\ (n44-n9){{\ghostlmd^{4}}}\ (n45-n8){{\ghostvar^{3}}}\ (n46-n7){{\ghostlmd^{3}}}\ (n47-n6){{\ghostvar^{5}}}\ (n48-n5){{\ghostlmd^{5}}}\ (n49-n4){{\ghostvar^{6}}}\ (n50-n3){{\ghostlmd^{6}}}\ (n51-n2){{\ghostvar^{4}}}\ (n52-n1){{\ghostlmd^{4}}}\ (n53-n0){{\ghostvar^{3}}}\ (n54-n53){{\ghostlmd^{1}}}\ (n55-n52){{\ghostvar^{1}}}\ (n56-n51){{\ghostlmd^{1}}}\ (n57-n50){{\ghostvar^{1}}}\ (n58-n49){{\ghostlmd^{1}}}\ (n59-n48){{\ghostvar^{1}}}\ (n60-n47){{\ghostlmd^{1}}}\ (n61-n46){{\ghostvar^{1}}}\ (n62-n45){{\ghostlmd^{1}}}\ (n63-n44){{\ghostvar^{1}}}\ (n64-n43){{\ghostlmd^{1}}}\ (n65-n42){{\ghostvar^{1}}}\ (n66-n41){{\ghostlmd^{1}}}\ (n67-n40){{\ghostvar^{1}}}\ (n68-n39){{\ghostlmd^{1}}}\ (n69-n38){{\ghostvar^{1}}}\ (n70-n37){{\ghostlmd^{1}}}\ (n71-n36){{\ghostvar^{1}}}\ (n72-n35){{\ghostlmd^{1}}}\ (n73-n34){{\ghostvar^{1}}}\ (n74-n33){{\ghostlmd^{1}}}\ (n75-n32){{\ghostvar^{1}}}\ (n76-n31){{\ghostlmd^{1}}}\ (n77-n30){{\ghostvar^{1}}}\ (n78-n29){{\ghostlmd^{1}}}\ (n79-n28){{\ghostvar^{1}}}\ (n80-n27){{\ghostlmd^{1}}}\ (n81-n26){{\ghostvar^{2}}}\ (n82-n25){{\ghostlmd^{2}}}\ (n83-n24){{\ghostvar^{2}}}\ (n84-n23){\lambda }\ (n85-n6){x}\ (n86-n5){{\ghostlmd^{3}}}\ (n87-n4){{\ghostvar^{4}}}\ (n88-n3){{\ghostlmd^{4}}}\ (n89-n2){{\ghostvar^{2}}}\ (n90-n1){{\ghostlmd^{2}}}\ (n91-n0){{\ghostvar^{1}}}\ (n92-n91){{\ghostlmd^{2}}}\ (n93-n90){{\ghostvar^{2}}}\ (n94-n89){{\ghostlmd^{2}}}\ (n95-n88){{\ghostvar^{2}}}\ (n96-n87){{\ghostlmd^{2}}}\ (n97-n86){{\ghostvar^{2}}}\ (n98-n85){\lambda }\ (n99-n22){z}\ (n100-n21){{\ghostlmd^{2}}}\ (n101-n20){{\ghostvar^{2}}}\ (n102-n19){\lambda }\ (n103-n10){x}\ (n104-n9){{\ghostlmd^{3}}}\ (n105-n8){{\ghostvar^{2}}}\ (n106-n7){{\ghostlmd^{2}}}\ (n107-n6){{\ghostvar^{4}}}\ (n108-n5){{\ghostlmd^{4}}}\ (n109-n4){{\ghostvar^{5}}}\ (n110-n3){{\ghostlmd^{5}}}\ (n111-n2){{\ghostvar^{3}}}\ (n112-n1){{\ghostlmd^{3}}}\ (n113-n0){{\ghostvar^{2}}}\ (n114-n113){{\ghostlmd^{2}}}\ (n115-n112){{\ghostvar^{2}}}\ (n116-n111){{\ghostlmd^{2}}}\ (n117-n110){{\ghostvar^{2}}}\ (n118-n109){{\ghostlmd^{2}}}\ (n119-n108){{\ghostvar^{2}}}\ (n120-n107){{\ghostlmd^{2}}}\ (n121-n106){{\ghostvar^{2}}}\ (n122-n105){{\ghostlmd^{2}}}\ (n123-n104){{\ghostvar^{2}}}\ (n124-n103){\lambda }\ (n125-n18){z}\ (n126-n17){{\ghostlmd^{2}}}\ (n127-n16){{\ghostvar^{2}}}\ (n128-n15){{\ghostlmd^{2}}}\ (n129-n14){{\ghostvar^{3}}}\ (n130-n13){{\ghostlmd^{3}}}\ (n131-n12){{\ghostvar^{3}}}\ (n132-n11){{\ghostlmd^{3}}}\ (n133-n10){{\ghostvar^{5}}}\ (n134-n9){{\ghostlmd^{5}}}\ (n135-n8){{\ghostvar^{4}}}\ (n136-n7){{\ghostlmd^{4}}}\ (n137-n6){{\ghostvar^{6}}}\ (n138-n5){{\ghostlmd^{6}}}\ (n139-n4){{\ghostvar^{7}}}\ (n140-n3){{\ghostlmd^{7}}}\ (n141-n2){{\ghostvar^{5}}}\ (n142-n1){{\ghostlmd^{5}}}\ (n143-n0){{\ghostvar^{4}}}}$}
\end{landscape}

Keeping only the maximal traversals gives us the following set of maximal normalizing traversals of $M$:
$$\travsetnorm(M) = \{ t_{11}, t_{121}, t_{122} \}$$

Hence the set of maximal paths in the beta-normal form of $M$ is given by the traversal core P-views:
\begin{align*}
\pview{t_{11} \filter \theroot} &=
    \Pstr[0.7cm]{(n0){\lambda }\ (n1-n0){{\ghostvar^{3}}}\ (n2-n1){{\ghostlmd^{1}}}\ (n3-n0){{\ghostvar^{1}}}\ (n4-n3){{\ghostlmd^{1}}}\ (n5-n0){{\ghostvar^{3}}}}
\\
\pview{t_{121} \filter \theroot} &=
    \Pstr[0.7cm]{(n0){\lambda }\ (n1-n0){{\ghostvar^{3}}}\ (n2-n1){{\ghostlmd^{1}}}\ (n3-n0){{\ghostvar^{1}}}\ (n4-n3){{\ghostlmd^{2}}}\ (n5-n0){{\ghostvar^{2}}}\ (n6-n5){{\ghostlmd^{1}}}\ (n7-n0){{\ghostvar^{3}}}}
\\
\pview{t_{122} \filter \theroot} &=
\Pstr[0.7cm]{(n0){\lambda }\ (n1-n0){{\ghostvar^{3}}}\ (n2-n1){{\ghostlmd^{1}}}\ (n3-n0){{\ghostvar^{1}}}\ (n4-n3){{\ghostlmd^{2}}}\ (n5-n0){{\ghostvar^{2}}}\ (n6-n5){{\ghostlmd^{2}}}\ (n7-n0){{\ghostvar^{4}}}}
\end{align*}

Therefore the beta-normal from of $varity\ 2$ is:
\begin{align*}
\betanf{varity\ 2} &= \lambda x_1 x_2 x_3 x_4 . x_3 (x_1 x_3 (x_2 x_3 x_4))\\
&\equiv_\alpha \lambda x y s z . s (x s (y s z))
\end{align*}
\end{example}

\section{Further directions}

\subsection{Higher-order collapsible pushdown automata}

In \cite{Ong2006}, a construction\footnote{This CPDA construction from HORS is implemented in the HOG tool.)}
 is defined that converts a higher-order recursion scheme into an equivalent collapsible higher-order pushdown automaton (in the sense that they generate the same infinite tree). The automaton proceeds by calculating traversals over the tree representation of the higher-order recursion scheme. In that setting, the recursion scheme represents a closed term (no free variable) of order-$0$, and the generated structure is an order-$0$ value (a tree).

Conceivably, there must be some kind of tree-generating automaton that can be used to calculate the set of traversals of an untyped lambda term as defined in this paper. The generated tree would be an order-$0$ tree representation of the beta-normal of a term: each terminal symbol of the automaton would correspond to a ``token'' of the lambda term: either a lambda node (represented by non-terminal of type $o->o$), or a variable node (represented by a non-terminal of type $o^k->o$ where $k\geq0$ is the number of operand applied to the variable in the beta normal form of the term.

In the untyped lambda calculus, though, there is no notion of order, therefore the type-based definition of higher-order pushdown automata, where each non-terminals has a simple-type, would not be appropriate here. The definition of HOCPDA would thus need to be relaxed to allow non-terminals of any type. The construction of the HOPCPDA itself would follow the same lines as \cite{Ong2006} except that $\eta$-long normal expansion is not needed.

\subsection{Connection with compilation of ULC to LLL}

In \cite{JonesBerezunLLL}, Daniil Berezun and Neil Jones define a different version of traversals with notable differences: no ghost lambda nodes, two types of justification pointers: binding pointers and control pointers. Although very similar, the relationship between their notion of traversals and the one defined in this paper is still not formally established. Danos and Regnier's Linear Head reduction paper \cite{danos-head} appears to be the best avenue to establish the connection.


\subsection{Game semantics connection}

In the lines of what I did in \cite{BlumPhd} on formalizing the isomorphism between the traversal theory and Game Semantics for the simply-typed lambda calculus, PCF and Idealized Algol, a natural question to ask is whether there is a similar connection between the ULC traversals defined in this paper on game models of the untyped lambda calculus.

TODO: give intuition on why this notion of traversals probably coincides with the game model of the untyped lambda calculus by Andrew Ker.

\subsection{Head linear reduction}

Traversals have emerged as a constructive method to represent
Relationship between Game Semantics and

In 2004, Danos and Regnier \cite{danos-head} establish the connection between Game Semantics and the notion of \emph{head-linear reduction}.

Head linear reduction is  a non-standard reduction strategy for the lambda calculus and shed some light studies the connection with Game Semantics.

\subsection{Notes for Neil and Daniil}

Here are some thoughts on comparing the following three approaches
based on the conversations Neil and I had during his visit to MSR:
(i) Berezun-Jones (BJ) traversals,
(ii) Danos-Regnier's Pointer Abstract Machine (PAM) state runs,
(iii) Blum's game-semantic-inspired imaginary traversals for ULC introduced in this note.

\subsubsection{Normalization}

\begin{itemize}
  \item Danos-Regnier: PAM implements head-linear reduction. This reduction does not itself normalize the term, it only yields the head-normal form of the term! The paper does not clearly indicate any method to normalize the term based solely on head-linear reduction.
      In particular in the PAM transition of Section 5.2, step 2, if the hoc variable is free then the machine stops: no transition applies.
  \item Berezun-Jones: Although not explicitly explained in the paper. Daniil and Neil have a ``read-out procedure''' to reconstruct the term from the traversal. The procedure is based on rewriting rules applied to the traversals.
  \item Blum: Normalization is performed by exploring all possible paths in the beta-normal form of the term. The algorithm generates one maximal traversal per path in the tree representation of the term. The term can then be uniquely recreated (up to variable naming) from those paths.
\end{itemize}

\subsubsection{Traversal/Run structure}
\begin{itemize}
\item Danos-Regnier: State run. Each state has three components: one pointer, a reference to a variable node, a reference to a subterm.
\item Berezun-Jones: Traversals of `tokens' with two pointers (control and binding pointers). A tokens is a reference to a node in the AST of the lambda term.
\item Blum: Traversals are sequences of node with one back pointer. Nodes can either be structural nodes from the `compact' AST of the term (where consecutive lambda nodes and application nodes are bundled together) or imaginary `ghost' nodes. Each node occurrence except the first one has exactly one justification pointer.
\end{itemize}

\subsubsection{Binding pointers -- PAM pointers -- P-view}

Neil-Daniil's binding pointers represent the ``head $\lambda$-list'' notion in Danos-Regnier paper (page 3): it gives the list of lambda nodes encountered from the variable all the way to the root.

\begin{itemize}
\item Danos-Regnier: The De Bruijn index of a variable is used to find the binder in the head $\lambda$-list. If index is $k$ (by Lemma 11) the binder can be retrieved by following $k$ pointers in the PAM states. The notion of ``subterm chain'' of a variable correspond to the notion of P-view in Game Semantics.

\item  Berezun-Jones: The De Bruijn index is used to determine how many binding (green) pointers to follow to reach the occurrence of the binder in the traversal.

\item Blum: Traversals are game-semantics inspired (ala Ong) and in particular verify the alternation property. To that end, consecutive binders are bulked together in a single binder node. Given a variable occurrence, its `bulk' binder is located by reading the P-view backward by $k'$ iterations where $k'$ is statically determined as the distance between the variable occurrence and its binder in the computation tree. The label associated with the link determines which exact variable in the `bulk lambda' binds the variable.
\end{itemize}

\subsubsection{Control pointers -- PAM -- }

\subsubsection{Retrieving application operands: Control pointers -- PAM's redex argument }
\begin{itemize}
  \item Danos-Regnier: the operands of a `prime redex' is statically defined from the term's expression (the `argument of a hoc variable', in Section 5.1)

  \item Berezun-Jones: control pointers are used to associate operators to the corresponding operand in an application: for instance if $@_1 (\lambda x .M) N$ is a redex of the tree then any occurrence of $\lambda x$ will point to some previous occurrence of $@_1$ in the traversal. The token representing the operand $N$ can then be statically determined from the AST of the tree.
      Control pointers always point to $@$ tokens.

  \item Blum: There are two cases:
     \begin{itemize}
     \item Structural operands: Same as Ong's traversals: pretty much the same as Berezun-Jones with the only technical difference that consecutive lambda and applications nodes are `bulked' together. In particular, `bulk lambda' nodes point to their parent node (either a variable or an application node $@$). If the variable $x$ of the redex is the $i$th bound variable in the bulk lambda, then the operand is obtained by fetching the $i$th child of the bulk lambda's parent node.
    \item Eta-expanded operands: When an operand is missing, a ghost lambda node is first introduced with rule \rulenamet{Var} (corresponding to the variable obtained after eta-expanding the operator). Then through repeated application of rule
        \rulenamet{Lam^\ghostlmd} and \rulenamet{Var}, either a structural node gets eventually materialized, which then represents the operand of the application; or the traversal reaches a ghost variable hereditarily justified by the root. In the latter case, a new variable gets created through eta-expansion to represent the missing operand.

        \emph{Note}: the ghost materialization loop implemented by rule \rulenamet{Lam^\ghostlmd} and \rulenamet{Var} proceeds by calculating the quantity ``$|\alpha|+i-|m|$'' at each iteration which is the exact same quantity ($r-a+l$) used in step 2. (b) of the PAM transition \cite{danos-head}, the ``price to pay for having no stacks or environments''.
    \end{itemize}

\end{itemize}


\subsubsection{Flag + Control pointers -- PAM's ``price to pay''  -- Ghost nodes}
\begin{itemize}
  \item Berezun-Jones: Semantics 2, 3, 4 and 5 from section 3 of \cite{JonesBerezunLLL} involve a `flag' boolean parameter that indicates the context of the application (either weak or strong evaluation).
  \item Danos-Regnier: The `flag' must somehow be present in the iterative loop in step 2(b) of the PAM transition--the ``price to pay for having no stacks or environments'' (Danos-Regnier).
  \item Blum: The `flag' must be somehow be hidden in the iterative ghost materialization algorithm implemented by the \rulenamet{Var} and \rulenamet{Lam^\ghostlmd} rules. (The ghost nodes being the equivalent of the ``price to pay'' from Danos-Regnier's paper.
\end{itemize}


\subsubsection{Converting other traversals to the imaginary ULC traversals}

\paragraph{From Berezun-Jones}

\begin{enumerate}
\item Remove single application token $@$ that are immediately followed by a variable.
\item Merge consecutive application nodes in the traversal, preserve all control pointers pointing to them. Assign link label $0$ if the source of the pointer is in operator position of the $@$, and $i >0$ if is in operand position of the $i$th merged $@$.

Remove all binding and control pointers starting from the $@$ tokens themselves.

\item Merge consecutive lambda tokens $\lambda x_1$, \ldots, $\lambda x_n$, $n>1$ in the traversals into single lambda nodes $\lambda x_1 \ldots x_n$.
 Binding pointers:
 \begin{itemize}
 \item For any binding pointer from some variable token $x$ in the traversal, pointing to one of the merged lambda nodes, replace the link as follows: if $x$ has deBrujin index $i$ then follow $i$ binding pointer in the original sequence. This will lead to some lambda token $\lambda x$. Make the link points instead to the merged lambda node containing that $\lambda x$. Assigned link label $i$ where $i$ is the index of the bound variable in the merged list of $x_1 \ldots x_n$.
 \item Remove all the binding pointers from either lambda or $@$ tokens that are pointing to any of the merged lambda nodes.
 \item Remove all binding pointers from the merged lambda nodes to other nodes.
 \end{itemize}
 Control pointers: preserve only the control pointer associated with the first lambda in the sequence of lambda tokens.

\item Add dummy lambdas $\lambda$ and ghost nodes: for each pair $(x,y)$ of consecutive variable tokens $x$ and $y$, processing the traversal from left to right, reconstruct the ``missing'' dummy and ghost nodes between $x$ and $y$ by applying rules \rulenamet{Lam^\normalizing} and \rulenamet{Var^\normalizing}.
\\

{\bf IT REMAINS TO PROVE THAT}: by applying those rules repeatedly starting from $x$, we necessarily get back to variable nodes $y$ with the expected justification pointer. The proof will involve formalizing the relationship between (i) flag  and control pointers (ii) ghost nodes.
\end{enumerate}

\paragraph{From Danos-Regnier}

Take a substitution sequence of a PAM: $((z_0,A_1), (z_1,A_2) \ldots$.
\todo{type notes}
%The traversals can be reconstructed as follows:

\section{Correctness proof (draft)}

\subsection{Background: Lambda calculus and reduction}
We recall from standard results of the lambda calculus.
A \defname{redex} is a sub-term of the form $(\lambda x. M) N$.
Reducing redex $(\lambda x. M) N$, or also \emph{firing} the redex, means substituting all free occurrences of $x$ in $M$ by the term $N$ in which all the free occurrences of $x$ are renamed with a fresh variable name (so as to avoid variable capture). We write this
$M[N[z/x]/x]$ where $z$ is a fresh variable name and $M[\_/\_]$ denotes capture permitting substitution.

A term is said to be in \defname{normal form} if it does not contain any redex.
A term is in \defname{head normal form} if it can be written $\lambda x_1 \ldots x_n . y A_1 \ldots A_m$ for $n,m\geq0$. If a term is not in head normal form then its \defname{head beta-redex} is the left-most redex, otherwise the term does not have any head beta-redex.

The \defname{head reduction}, denoted $\rightarrow_{h}$, fires the head redex of a term if it exists. It can be shown that its transitive closure $\rightarrow^+_{h}$ yields the head-normal form. The \defname{normal-order reduction strategy}, also called leftmost-outermost reduction strategy, performs head reduction until reaching the head-normal form and then recursively applies head reduction on each operand of the head variable. This reduction strategy can be shown to yield the normal form if it exists.

For any reduction relation $\rightarrow$ between terms we will write
$\rightarrow^+$ to denote its transitive closure.

\subsection{Head-linear reduction}
We now introduce the head-linear reduction of Danos-Regnier \cite{danos-head}.

In the standard lambda calculus, a redex is necessarily formed by the outermost lambda in operator position of an application: if the operator consists of consecutive lambda abstractions (\eg, as in $(\lambda x \lambda y . M) A_1 A_2$) then the outermost lambda (\eg, $\lambda x$) is the one that will form the redex. The notion of redex can be generalized to allow evaluation of arguments in any orders. In particular, one can allow any of the consecutive $\lambda$-abstractions to form a redex (\eg, the abstraction $\lambda y$ and corresponding argument $A_2$ would be a valid redex). This is formally as follows:

\begin{definition}[Generalized redex]
The set of generalized redexes of a term $M$, written $gr(M)$, is a set of pairs $(\lambda x, A)$ where $\lambda x$ is some abstraction in $M$ and $A$ is a subterm called the argument of $\lambda x$. The head $\lambda$ list of $M$, written $\lambda_l(M)$ is a list of lambda abstractions of $M$. They are defined by induction as follows:
\begin{align*}
gr(v) &= \emptyset & \lambda_l(v) &= \emptyset\\
gr(\lambda x. U) &= gr(N) & \lambda_l(\lambda x. U) &= \lambda x \cdot \lambda_l(U) \\
gr(U V) &= \{ (\lambda x, V) \} \union gr(U) \union gr(V) &
\lambda_l(U V) &= l & \mbox{if $\lambda_l(U) = \lambda x \cdot l$} \\
gr(U V) &= gr(U) \union gr(V) & \lambda_l(U V) &= \epsilon & \mbox{if $\lambda_l(U) = \epsilon$.} \\
\end{align*}
where $v$ ranges over variable occurrences, $x$ ranges over variable names, $U, V$ range over subterms of $M$, and $\epsilon$ denotes the empty list.
\end{definition}

\begin{example} For any term $M, N, A_1, A_2$ we have
(i) $gr((\lambda x \lambda y . M) A_1 A_2) = \{ (\lambda x, A_1), (\lambda y, A_2)\}$ and
(ii) $gr((\lambda z . (\lambda x \lambda y . M) N) A_1 A_2) = \{ (\lambda z, A_1), (\lambda x, N), (\lambda y, A_2)\}$.
\end{example}

Note: In order to define head-linear reduction, one needs to consider specific \emph{occurrences} of variables and sub-terms in a given term. In particular, let's emphasize that when denoting a generalized redex by a pair $(\lambda x, A)$, the component $\lambda x$ and $A$ denote specific \emph{occurrences} of $\lambda x$ and subterm $A$.

We say that a variable occurrence is \defname{involved} in the generalized redex $(\lambda x, A)$ if the variable occurrence is bound by $\lambda x$. A variable occurrence can therefore be involved in at most one generalized redex. We define the \defname{linear substitution} of $x$ for $A$, written $M\{x\mapsto A\}$ as the term obtained by performing capture-avoiding substitution of that single occurrence of $x$ by $A$. (Compare this to the standard substitution that applies to \emph{every} occurrence of $x$ in $M$.) When performing such substitution we say that we \defname{linearly fire} the generalized redex $(\lambda x, A)$ for that occurrence of $x$.

We define the \defname{head variable occurrence} of a term as the left-most variable occurring in the term (\ie, the first variable found by depth-first traversal of the term tree.) If the head variable occurrence of a term is involved in a generalized redex then we call that redex the \defname{head-linear redex}.
A term that does not have a head-linear redex is said to be in \defname{quasi head-normal form}.

The \defname{head-linear reduction} $\rightarrow_{hl}$ is defined as the reduction that linearly fires the head linear redex, if it exists. It can be shown that the transitive closure $\rightarrow^+_{hl}$ yields the quasi head-normal form.

\paragraph{Soundness}
The set of \defname{spine subterms} is defined by induction: a term is a spine subterm of itself; the spine subterms of $U V$ and $\lambda x. U$ are those of $U$.
A \defname{spine prime redex} is a generalized redex $(\lambda x, A)$ such that the operator of the redex ($\lambda x . U$ for some $U$) is a spine subterm.

Danos-Reigner showed the following result:
\begin{theorem}[Soundness and completeness of head-linear reduction \cite{danos-head}]
\label{thm:danosreigner_headlinred}
\begin{compactitem}
\item If $T \rightarrow^+_{hl} T$  then $T$ and $T'$ are $\beta$-equivalent.
\item If $T$ is in quasi-head normal form and has $n$ spine prime redexes then the head reduction of $T$ leads to a head normal form in exactly $n$ steps.
\item If $T$ is any term, the head linear reduction of $T$ terminates iff the head reduction of $T$ terminates.
\end{compactitem}
\end{theorem}

\subsubsection{Leftmost linear reduction strategy}

As the name indicates, the head-linear reduction is a linear version of the \emph{head} reduction. It thus only yields the quasi \emph{head}-normal form, not the normal form, and therefore is not complete for normalization.

In the standard lambda calculus, normal-order reduction strategy is obtained by repeatedly applying head reduction to get to the head-normal form, and then continuously applying the head-reduction on each argument of the head variable.
This ultimately yields the normal form of the term if it exists.

We now define the linear counterpart of the normal-order reduction: Informally, the \defname{leftmost linear reduction strategy} is the strategy that performs head-linear reduction if possible, and otherwise, if the term is in quasi-head normal form, continuously (and recursively) applies the head-linear reduction on each argument (from left to right) of the head variable occurrence.
Formally:
\begin{definition}[Leftmost linear reduction]
Given a term $M$, we define the partial function $lloc_M$ from subterms of $M$ to variable occurrences by induction on the subterms of $M$:
\begin{align*}
lloc_M(v) &=
    \begin{cases}
    v &\mbox{if $v$ is involved in a generalized redex in $M$,} \\
    \bot & \mbox {otherwise.}
    \end{cases}  \\
lloc_M(\lambda x . U) &= lloc_M(U) \\
lloc_M(U V) &= \begin{cases}
                lloc_M(U) &\mbox{if $lloc_M(U)\neq\bot$,} \\
                lloc_M(V) &\mbox{if $lloc_M(U)=\bot$ is undefined and $\lambda_l(U) = \epsilon$.} \\
                \bot & \mbox{otherwise.}
              \end{cases}
\end{align*}
where $v$ ranges over variable occurrences in $M$,
and for any subterm $N$, $lloc_M(N) = \bot$ denotes that $lloc$ is undefined at $N$.

The \defname{leftmost linear variable occurrence} of $M$
is defined as $lloc_M(M)$ if it exists, otherwise $M$ is said to be in \defname{quasi normal form}. The \defname{leftmost linear reduction} strategy, written $\rightarrow_l$, is defined as the strategy that linearly fires the generalized redex involving the leftmost linear variable occurrence.
\end{definition}

Leftmost linear reduction proceeds by first locating the left-most variable occurrence and, if it is involved in a redex, linearly fires it. In comparison, the traditional left-most outermost reduction first locates the leftmost redex and then fires it by substituting all the variables involved in it.

\begin{example}
Take $M = (\lambda x. x x N) z$. We have $M \rightarrow_{hl} (\lambda x. z x N) z$ which is in quasi head normal form because the head variable $z$ is not involved in any generalized redex.
But since the left-most occurrence $x$ is involved in the redex $(\lambda x, z)$ the leftmost linear reduction gives $(\lambda x. z x N) z \rightarrow_l (\lambda x. z z N) z$.
\end{example}

The definition of prime redex from Danos-Regnier corresponds to the \emph{prime spine redexes} in our setting. We generalize the notion of prime redex as follows
\begin{definition}[Prime redexes]
The set of prime redexes $PR(M)$ of a term $M$ is a subset of the generalized redexes of $M$ defined by induction as follows:
\begin{align*}
PR(v) &= \emptyset \\
PR(\lambda x . U) &= PR(U)\\
PR(U V) &=
    \begin{cases}
        \{(\lambda x, V) \} \union PR(U) & \mbox{if $\lambda_l(U) = \lambda x \cdot l $} \\
        PR(U) & \mbox{if $\lambda_l(U) = \epsilon$ and $lloc_M(U)$ is defined} \\
        PR(U) \union PR(V) & \mbox{if $\lambda_l(U) = \epsilon$ and $lloc_M(U)$ is undefined}
    \end{cases}
\end{align*}
where $v$ ranges over variable occurrences and $U,V$ over subterms of $M$.
\end{definition}

\begin{example}
Take $M = (\lambda x . z ((\lambda w y . y)x)) U$ for any term $U$.
Then $M$ is in quasi-head normal form with one prime spine redex $(x,U)$.
Head reduction gives $M \rightarrow_h z ((\lambda w y. y) U)$.

$M$ is also in quasi normal form. Indeed $lloc_M(M) = lloc_M(N) = lloc_M (\lambda w y . y)$ which is undefined.
The prime redexes of $M$ are $PR(M) = \{ (\lambda x, U) , (\lambda w, x) \}$.
Reducing the prime redexes with two head reduction gives
$M \rightarrow_h z ((\lambda w y.y)x) \rightarrow_h z (\lambda y . y)$ which is in normal form.
\end{example}


\subsection{Traversal Correctness}

Proof idea: ``traversals implement left-most linear reduction''.
The proof sketch:
\begin{itemize}
\item Show that `leftmost linear reduction' yields the `quasi normal form'.  (Should not be too difficult: a consequence of Danos-Regnier’s Theorem 2)
\item Observe that if $M$ is beta-normal then (trivially) its set of traversal cores P-views is precisely the set of paths in the tree representation of M
\item If $M$ reduces to $M'$ using the linear reduction then the traversal cores P-views of $M'$ are isomorphic to the traversals core P-views of $M$.
\item Reducing a trivial redex preserves (modulo some traversal prefix) the set of traversals.
The interesting part of the proof will be to show how the ghost nodes gradually ``materialize'' after each step of the linear reduction.
\end{itemize}

\bibliographystyle{abbrv}
\bibliography{ulctrav}

\end{document}
